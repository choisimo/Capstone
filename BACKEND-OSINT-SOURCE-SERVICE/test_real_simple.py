#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ì‹¤ì œ ë°ì´í„° í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import json
from datetime import datetime

def test_real_data():
    """ì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"""
    
    results = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print("=" * 60)
    print("ğŸš€ ì‹¤ì œ ë°ì´í„° í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ë©”ì¸ í˜ì´ì§€
    print("\n1ï¸âƒ£ êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ë©”ì¸í˜ì´ì§€ í…ŒìŠ¤íŠ¸...")
    try:
        url = "https://www.nps.or.kr"
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = soup.find('title')
        print(f"   âœ… ì‚¬ì´íŠ¸ ì œëª©: {title.text if title else 'Not found'}")
        
        # ë©”ì¸ ë©”ë‰´ í•­ëª©ë“¤ ì°¾ê¸°
        menus = soup.find_all('a', limit=10)
        menu_texts = [m.get_text(strip=True) for m in menus if m.get_text(strip=True)]
        print(f"   âœ… ë©”ë‰´ í•­ëª© {len(menu_texts)}ê°œ ë°œê²¬")
        
        for i, text in enumerate(menu_texts[:5], 1):
            if text:
                print(f"      - {text}")
        
        results.append({
            'site': 'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨',
            'url': url,
            'status': 'success',
            'title': title.text if title else None,
            'items_found': len(menu_texts)
        })
        
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        results.append({
            'site': 'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨',
            'url': url,
            'status': 'error',
            'error': str(e)
        })
    
    # 2. ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ 'êµ­ë¯¼ì—°ê¸ˆ' ê²€ìƒ‰
    print("\n2ï¸âƒ£ ë„¤ì´ë²„ ë‰´ìŠ¤ 'êµ­ë¯¼ì—°ê¸ˆ' ê²€ìƒ‰...")
    try:
        url = "https://search.naver.com/search.naver?where=news&query=êµ­ë¯¼ì—°ê¸ˆ&sort=0&photo=0&field=0&pd=1"
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ì œëª© ì°¾ê¸°
        news_titles = soup.select('a.news_tit')
        
        print(f"   âœ… ë‰´ìŠ¤ {len(news_titles)}ê°œ ë°œê²¬")
        
        articles = []
        for i, title in enumerate(news_titles[:5], 1):
            title_text = title.get_text(strip=True)
            link = title.get('href', '')
            print(f"   ğŸ“° ê¸°ì‚¬ {i}: {title_text[:60]}...")
            articles.append({
                'title': title_text,
                'url': link
            })
        
        results.append({
            'site': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
            'url': url,
            'status': 'success',
            'query': 'êµ­ë¯¼ì—°ê¸ˆ',
            'news_count': len(news_titles),
            'sample_articles': articles
        })
        
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        results.append({
            'site': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
            'status': 'error',
            'error': str(e)
        })
    
    # 3. êµ¬ê¸€ ë‰´ìŠ¤ RSS (í•œêµ­)
    print("\n3ï¸âƒ£ êµ¬ê¸€ ë‰´ìŠ¤ RSS (êµ­ë¯¼ì—°ê¸ˆ)...")
    try:
        url = "https://news.google.com/rss/search?q=êµ­ë¯¼ì—°ê¸ˆ+when:1d&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(url)
        
        print(f"   âœ… RSS í•­ëª© {len(feed.entries)}ê°œ ë°œê²¬")
        
        rss_items = []
        for i, entry in enumerate(feed.entries[:5], 1):
            print(f"   ğŸ“„ ë‰´ìŠ¤ {i}: {entry.title[:60]}...")
            print(f"      ì¶œì²˜: {entry.source.title if hasattr(entry, 'source') else 'Unknown'}")
            print(f"      ë§í¬: {entry.link[:50]}...")
            rss_items.append({
                'title': entry.title,
                'link': entry.link,
                'published': entry.get('published', 'Unknown')
            })
        
        results.append({
            'site': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
            'url': url,
            'status': 'success',
            'feed_count': len(feed.entries),
            'sample_items': rss_items
        })
        
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        results.append({
            'site': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
            'status': 'error',
            'error': str(e)
        })
    
    # 4. ì—°í•©ë‰´ìŠ¤ ê²½ì œ ì„¹ì…˜
    print("\n4ï¸âƒ£ ì—°í•©ë‰´ìŠ¤ ê²½ì œ ì„¹ì…˜...")
    try:
        url = "https://www.yna.co.kr/economy/all"
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ì‚¬ ì œëª© ì°¾ê¸°
        articles = soup.select('div.news-con strong.news-tl a')
        
        print(f"   âœ… ê²½ì œ ê¸°ì‚¬ {len(articles)}ê°œ ë°œê²¬")
        
        article_list = []
        for i, article in enumerate(articles[:5], 1):
            title = article.get_text(strip=True)
            link = article.get('href', '')
            if link and not link.startswith('http'):
                link = f"https://www.yna.co.kr{link}"
            print(f"   ğŸ“° ê¸°ì‚¬ {i}: {title[:60]}...")
            article_list.append({
                'title': title,
                'url': link
            })
        
        results.append({
            'site': 'ì—°í•©ë‰´ìŠ¤',
            'url': url,
            'status': 'success',
            'articles_count': len(articles),
            'sample_articles': article_list
        })
        
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        results.append({
            'site': 'ì—°í•©ë‰´ìŠ¤',
            'status': 'error',
            'error': str(e)
        })
    
    # 5. ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰
    print("\n5ï¸âƒ£ ë‹¤ìŒ ë‰´ìŠ¤ 'êµ­ë¯¼ì—°ê¸ˆ' ê²€ìƒ‰...")
    try:
        url = "https://search.daum.net/search?w=news&q=êµ­ë¯¼ì—°ê¸ˆ&DA=YZR&spacing=0"
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ì œëª© ì°¾ê¸°
        news_items = soup.select('a.tit_main')
        
        print(f"   âœ… ë‰´ìŠ¤ {len(news_items)}ê°œ ë°œê²¬")
        
        articles = []
        for i, item in enumerate(news_items[:5], 1):
            title = item.get_text(strip=True)
            link = item.get('href', '')
            print(f"   ğŸ“° ê¸°ì‚¬ {i}: {title[:60]}...")
            articles.append({
                'title': title,
                'url': link
            })
        
        results.append({
            'site': 'ë‹¤ìŒ ë‰´ìŠ¤',
            'url': url,
            'status': 'success',
            'query': 'êµ­ë¯¼ì—°ê¸ˆ',
            'news_count': len(news_items),
            'sample_articles': articles
        })
        
    except Exception as e:
        print(f"   âŒ ì˜¤ë¥˜: {e}")
        results.append({
            'site': 'ë‹¤ìŒ ë‰´ìŠ¤',
            'status': 'error',
            'error': str(e)
        })
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    success_count = sum(1 for r in results if r.get('status') == 'success')
    total_items = sum(
        r.get('items_found', 0) + 
        r.get('news_count', 0) + 
        r.get('feed_count', 0) + 
        r.get('articles_count', 0)
        for r in results
    )
    
    print(f"âœ… ì„±ê³µ: {success_count}/{len(results)} ì‚¬ì´íŠ¸")
    print(f"ğŸ“„ ì´ ìˆ˜ì§‘ í•­ëª©: {total_items}ê°œ")
    
    for result in results:
        if result.get('status') == 'success':
            site = result.get('site')
            count = (result.get('items_found', 0) + 
                    result.get('news_count', 0) + 
                    result.get('feed_count', 0) + 
                    result.get('articles_count', 0))
            print(f"   âœ… {site}: {count} items")
        else:
            print(f"   âŒ {result.get('site')}: Failed")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = f"real_data_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_file}")
    
    # ì‹¤ì œ ë°ì´í„° ê²€ì¦
    print("\n" + "=" * 60)
    if success_count > 0 and total_items > 0:
        print("âœ… ê²€ì¦ ì™„ë£Œ: ì‹¤ì œ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("âœ… Mock ë°ì´í„° ì—†ìŒ - ëª¨ë“  ë°ì´í„°ëŠ” ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    print("=" * 60)
    
    return results

if __name__ == "__main__":
    results = test_real_data()
