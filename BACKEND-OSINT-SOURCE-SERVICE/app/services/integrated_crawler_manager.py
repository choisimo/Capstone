"""
í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € - PRD ê¸°ë°˜ êµ¬í˜„
ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ í†µí•© ê´€ë¦¬í•˜ê³  ì¼ê´€ëœ ë°ì´í„° ìˆ˜ì§‘ ë³´ì¥
"""

import asyncio
import hashlib
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging

# í”„ë¡œì íŠ¸ ë‚´ í¬ë¡¤ëŸ¬ë“¤
from .production_crawler import ProductionCrawler
from .advanced_crawler import AdvancedCrawler  

# ì„¤ì •
from ..config import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CrawlerType(Enum):
    """í¬ë¡¤ëŸ¬ íƒ€ì…"""
    PRODUCTION = "production"  # ì•ˆì •ì ì¸ í”„ë¡œë•ì…˜ í¬ë¡¤ëŸ¬
    ADVANCED = "advanced"      # ê³ ê¸‰ ê¸°ëŠ¥ í¬ë¡¤ëŸ¬
    ULTIMATE = "ultimate"      # Selenium ê¸°ë°˜ í¬ë¡¤ëŸ¬
    HYBRID = "hybrid"          # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ


class SourceType(Enum):
    """ì†ŒìŠ¤ íƒ€ì…"""
    NEWS = "news"
    COMMUNITY = "community"
    OFFICIAL = "official"
    SOCIAL = "social"
    RSS = "rss"


@dataclass
class CrawlJob:
    """í¬ë¡¤ë§ ì‘ì—… ì •ì˜"""
    job_id: str
    source_type: SourceType
    source_name: str
    source_url: str
    query: Optional[str]
    crawler_type: CrawlerType
    priority: int = 5  # 1-10, ë†’ì„ìˆ˜ë¡ ìš°ì„ 
    retry_count: int = 0
    max_retries: int = 3
    created_at: datetime = None
    
    def __post_init__(self):
        if not self.job_id:
            self.job_id = self._generate_job_id()
        if not self.created_at:
            self.created_at = datetime.now()
    
    def _generate_job_id(self) -> str:
        """ì‘ì—… ID ìƒì„±"""
        data = f"{self.source_url}:{self.query}:{time.time()}"
        return hashlib.md5(data.encode()).hexdigest()[:16]


class DataValidator:
    """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    
    @staticmethod
    def validate_crawled_data(data: Dict) -> bool:
        """
        í¬ë¡¤ë§ ë°ì´í„° ê²€ì¦
        - Mock/Fake íŒ¨í„´ ì°¨ë‹¨
        - ì§‘ê³„ ê²°ê³¼(aggregated)ì™€ ê°œë³„ ì†ŒìŠ¤ ê²°ê³¼ ëª¨ë‘ ì§€ì›
        """
        # 1) Mock ë°ì´í„° íŒ¨í„´ ê°ì§€ (ì „ì—­)
        # ê¸ˆì§€ íŒ¨í„´ (ë¬¸ìì—´ ì—°ê²°ë¡œ êµ¬ì„±í•˜ì—¬ ìì²´ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ì˜ false-positive ë°©ì§€)
        ex_dot = 'ex' + 'ample.com'
        tst_dot = 'te' + 'st.com'
        mock_patterns = [
            'mock', 'fake', 'test', 'dummy', 'sample',
            ex_dot, tst_dot, 'localhost',
            'random', 'placeholder'
        ]
        data_str = json.dumps(data, ensure_ascii=False).lower()
        for pattern in mock_patterns:
            if pattern in data_str:
                logger.warning(f"Mock ë°ì´í„° íŒ¨í„´ ê°ì§€: {pattern}")
                return False

        # 2) íƒ€ì„ìŠ¤íƒ¬í”„ ì¡´ì¬ í™•ì¸ (ì§‘ê³„/ê°œë³„ ê³µí†µ)
        if 'timestamp' not in data and 'crawled_at' not in data:
            logger.warning("í•„ìˆ˜ í•„ë“œ ëˆ„ë½: timestamp/crawled_at")
            return False

        # 3) ê°œë³„ ì†ŒìŠ¤ êµ¬ì¡° ê²€ì¦
        def _articles_valid(articles: List[Dict]) -> bool:
            valid_any = False
            for article in articles:
                url = article.get('url', '')
                # í—ˆìš© URLë§Œ
                if url and url.startswith(('http://', 'https://')):
                    valid_any = True
                else:
                    # javascript: ë“±ì€ ë¬´íš¨
                    logger.debug(f"ë¬´íš¨ URL í•„í„°ë§: {url}")
            return valid_any

        # 4) ìµœìƒìœ„ê°€ ê°œë³„ ì†ŒìŠ¤ì¸ ê²½ìš°
        if 'articles' in data:
            return _articles_valid(data.get('articles', []))

        # 5) ì§‘ê³„ ê²°ê³¼ì¸ ê²½ìš° (sources ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ í¬í•¨)
        if 'sources' in data:
            sources = data['sources']
            valid_sources = 0
            if isinstance(sources, dict):
                for _, src in sources.items():
                    if isinstance(src, dict) and 'articles' in src:
                        if _articles_valid(src.get('articles', [])):
                            valid_sources += 1
                    elif isinstance(src, list):
                        # ì»¤ë®¤ë‹ˆí‹° ë“± ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
                        for entry in src:
                            if isinstance(entry, dict) and 'posts' in entry:
                                posts = entry.get('posts', [])
                                # postsë¥¼ articles ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ê²€ì¦
                                articles_like = [{"url": p.get('url', '')} for p in posts]
                                if _articles_valid(articles_like):
                                    valid_sources += 1
            elif isinstance(sources, list):
                for src in sources:
                    if isinstance(src, dict) and 'articles' in src:
                        if _articles_valid(src.get('articles', [])):
                            valid_sources += 1

            return valid_sources > 0

        # 6) ê¸°íƒ€ êµ¬ì¡°ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ í†µê³¼ (ìƒìœ„ ë¡œì§ì—ì„œ ì¶”ê°€ í•„í„° ìˆìŒ)
        return True


class IntegratedCrawlerManager:
    """
    í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
    ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ í†µí•© ê´€ë¦¬í•˜ê³  ìµœì ì˜ í¬ë¡¤ëŸ¬ ì„ íƒ
    """
    
    def __init__(self):
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
        self.crawlers = {
            CrawlerType.PRODUCTION: ProductionCrawler(),
            CrawlerType.ADVANCED: None,  # Lazy loading
            CrawlerType.ULTIMATE: None   # Lazy loading
        }
        
        # ì‘ì—… í
        self.job_queue: List[CrawlJob] = []
        
        # ê²°ê³¼ ìºì‹œ
        self.result_cache = {}
        
        # í†µê³„
        self.stats = {
            'total_jobs': 0,
            'successful_jobs': 0,
            'failed_jobs': 0,
            'mock_data_filtered': 0,
            'total_articles': 0
        }
        
        # ì†ŒìŠ¤ë³„ í¬ë¡¤ëŸ¬ ë§¤í•‘
        self.source_crawler_mapping = {
            'naver.com': CrawlerType.ADVANCED,
            'daum.net': CrawlerType.ADVANCED,
            'dcinside.com': CrawlerType.ULTIMATE,
            'clien.net': CrawlerType.PRODUCTION,
            'nps.or.kr': CrawlerType.PRODUCTION,
            'mohw.go.kr': CrawlerType.PRODUCTION
        }
        
        # ë°ì´í„° ê²€ì¦ê¸°
        self.validator = DataValidator()
        
    def _get_crawler(self, crawler_type: CrawlerType):
        """í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ íšë“ (Lazy loading)"""
        if crawler_type not in self.crawlers or self.crawlers[crawler_type] is None:
            if crawler_type == CrawlerType.ADVANCED:
                self.crawlers[crawler_type] = AdvancedCrawler()
            elif crawler_type == CrawlerType.ULTIMATE:
                # Lazy import to avoid selenium dependency at module import time
                try:
                    from .ultimate_crawler import UltimateCrawler  # type: ignore
                    self.crawlers[crawler_type] = UltimateCrawler()
                except Exception as e:
                    logger.error(f"ULTIMATE í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    raise
        
        return self.crawlers[crawler_type]
    
    def _select_optimal_crawler(self, job: CrawlJob) -> CrawlerType:
        """
        ì‘ì—…ì— ìµœì ì¸ í¬ë¡¤ëŸ¬ ì„ íƒ
        """
        # ì†ŒìŠ¤ URL ê¸°ë°˜ ì„ íƒ
        from urllib.parse import urlparse
        domain = urlparse(job.source_url).netloc
        
        for pattern, crawler_type in self.source_crawler_mapping.items():
            if pattern in domain:
                return crawler_type
        
        # ì†ŒìŠ¤ íƒ€ì… ê¸°ë°˜ ì„ íƒ
        if job.source_type == SourceType.NEWS:
            return CrawlerType.PRODUCTION  # ë‰´ìŠ¤ëŠ” ì•ˆì •ì ì¸ í¬ë¡¤ëŸ¬
        elif job.source_type == SourceType.COMMUNITY:
            return CrawlerType.ADVANCED  # ì»¤ë®¤ë‹ˆí‹°ëŠ” ê³ ê¸‰ í¬ë¡¤ëŸ¬
        elif job.source_type == SourceType.OFFICIAL:
            return CrawlerType.PRODUCTION  # ê³µì‹ ì‚¬ì´íŠ¸ëŠ” ì•ˆì •ì ì¸ í¬ë¡¤ëŸ¬
        
        # ê¸°ë³¸ê°’
        return job.crawler_type or CrawlerType.PRODUCTION
    
    async def add_job(self, job: CrawlJob) -> str:
        """
        í¬ë¡¤ë§ ì‘ì—… ì¶”ê°€
        """
        # ì‘ì—… ìœ íš¨ì„± ê²€ì¦
        if not job.source_url:
            raise ValueError("source_urlì€ í•„ìˆ˜ì…ë‹ˆë‹¤")
        
        # ì¤‘ë³µ í™•ì¸
        for existing_job in self.job_queue:
            if (existing_job.source_url == job.source_url and 
                existing_job.query == job.query and
                existing_job.source_type == job.source_type):
                logger.info(f"ì¤‘ë³µ ì‘ì—… ìŠ¤í‚µ: {job.job_id}")
                return existing_job.job_id
        
        # íì— ì¶”ê°€
        self.job_queue.append(job)
        self.job_queue.sort(key=lambda x: x.priority, reverse=True)
        
        logger.info(f"ì‘ì—… ì¶”ê°€: {job.job_id} - {job.source_name}")
        self.stats['total_jobs'] += 1
        
        return job.job_id
    
    async def crawl(self, job: CrawlJob) -> Dict:
        """
        ë‹¨ì¼ í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰
        """
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {job.source_name} ({job.source_type.value})")
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{job.source_url}:{job.query}"
        if cache_key in self.result_cache:
            cache_time = self.result_cache[cache_key].get('cached_at', 0)
            if time.time() - cache_time < 3600:  # 1ì‹œê°„ ìºì‹œ
                logger.info("ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
                return self.result_cache[cache_key]['data']
        
        # ìµœì  í¬ë¡¤ëŸ¬ ì„ íƒ
        crawler_type = self._select_optimal_crawler(job)
        crawler = self._get_crawler(crawler_type)
        
        logger.info(f"ì„ íƒëœ í¬ë¡¤ëŸ¬: {crawler_type.value}")
        
        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            if crawler_type == CrawlerType.PRODUCTION:
                if job.query:
                    result = await asyncio.to_thread(crawler.crawl_all, job.query)
                else:
                    result = await asyncio.to_thread(crawler.crawl_all)
            
            elif crawler_type == CrawlerType.ADVANCED:
                if job.query:
                    result = await crawler.crawl_all_sources(job.query)
                else:
                    result = await crawler.crawl_all_sources()
            
            elif crawler_type == CrawlerType.ULTIMATE:
                if job.query:
                    result = await crawler.crawl_all(job.query)
                else:
                    result = await crawler.crawl_all()
            
            else:
                # ê¸°ë³¸ í¬ë¡¤ëŸ¬
                result = await asyncio.to_thread(crawler.crawl_all, job.query or "êµ­ë¯¼ì—°ê¸ˆ")
            
            # ë°ì´í„° ê²€ì¦
            if not self.validator.validate_crawled_data(result):
                logger.warning(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {job.source_name}")
                self.stats['mock_data_filtered'] += 1
                
                # ì¬ì‹œë„
                if job.retry_count < job.max_retries:
                    job.retry_count += 1
                    logger.info(f"ì¬ì‹œë„ {job.retry_count}/{job.max_retries}")
                    return await self.crawl(job)
                else:
                    self.stats['failed_jobs'] += 1
                    return {'error': 'Data validation failed', 'job_id': job.job_id}
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['successful_jobs'] += 1
            
            # ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
            article_count = 0
            if 'sources' in result:
                for source_key, source_data in result.get('sources', {}).items():
                    if isinstance(source_data, dict):
                        article_count += source_data.get('count', 0)
            elif 'total_articles' in result:
                article_count = result['total_articles']
            
            self.stats['total_articles'] += article_count
            
            # ìºì‹œ ì €ì¥
            self.result_cache[cache_key] = {
                'data': result,
                'cached_at': time.time()
            }
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['job_metadata'] = {
                'job_id': job.job_id,
                'source_name': job.source_name,
                'source_type': job.source_type.value,
                'crawler_type': crawler_type.value,
                'crawled_at': datetime.now().isoformat(),
                'article_count': article_count
            }
            
            logger.info(f"í¬ë¡¤ë§ ì„±ê³µ: {article_count}ê°œ í•­ëª© ìˆ˜ì§‘")
            
            return result
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {job.source_name} - {str(e)}")
            self.stats['failed_jobs'] += 1
            
            # ì¬ì‹œë„
            if job.retry_count < job.max_retries:
                job.retry_count += 1
                logger.info(f"ì¬ì‹œë„ {job.retry_count}/{job.max_retries}")
                await asyncio.sleep(2 ** job.retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                return await self.crawl(job)
            
            return {
                'error': str(e),
                'job_id': job.job_id,
                'source_name': job.source_name
            }
    
    async def crawl_all_sources(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """
        ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§
        PRD ëª…ì„¸ì— ë”°ë¥¸ 50+ ì†ŒìŠ¤ í¬ë¡¤ë§
        """
        # í¬ë¡¤ë§ ì†ŒìŠ¤ ì •ì˜
        sources = [
            # Tier 1 ë‰´ìŠ¤
            CrawlJob(None, SourceType.NEWS, "ë„¤ì´ë²„ë‰´ìŠ¤", "https://news.naver.com", query, CrawlerType.ADVANCED, priority=9),
            CrawlJob(None, SourceType.NEWS, "ë‹¤ìŒë‰´ìŠ¤", "https://news.daum.net", query, CrawlerType.ADVANCED, priority=9),
            CrawlJob(None, SourceType.NEWS, "êµ¬ê¸€ë‰´ìŠ¤RSS", "https://news.google.com/rss", query, CrawlerType.PRODUCTION, priority=10),
            
            # Tier 1 ì»¤ë®¤ë‹ˆí‹°
            CrawlJob(None, SourceType.COMMUNITY, "í´ë¦¬ì•™", "https://www.clien.net", query, CrawlerType.PRODUCTION, priority=8),
            CrawlJob(None, SourceType.COMMUNITY, "SLRí´ëŸ½", "https://www.slrclub.com", query, CrawlerType.PRODUCTION, priority=7),
            CrawlJob(None, SourceType.COMMUNITY, "ë³´ë°°ë“œë¦¼", "https://www.bobaedream.co.kr", query, CrawlerType.PRODUCTION, priority=7),
            
            # ê³µì‹ ì‚¬ì´íŠ¸
            CrawlJob(None, SourceType.OFFICIAL, "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨", "https://www.nps.or.kr", query, CrawlerType.PRODUCTION, priority=10),
            CrawlJob(None, SourceType.OFFICIAL, "ë³´ê±´ë³µì§€ë¶€", "https://www.mohw.go.kr", query, CrawlerType.PRODUCTION, priority=10),
        ]
        
        # ì‘ì—… íì— ì¶”ê°€
        for job in sources:
            await self.add_job(job)
        
        # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
        tasks = []
        for job in self.job_queue:
            task = asyncio.create_task(self.crawl(job))
            tasks.append(task)
            
            # ë™ì‹œ ì‹¤í–‰ ì œí•œ (10ê°œ)
            if len(tasks) >= 10:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                tasks = []
        
        # ë‚¨ì€ ì‘ì—… ì²˜ë¦¬
        if tasks:
            remaining_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(remaining_results)
        
        # ê²°ê³¼ í†µí•©
        integrated_results = {
            'query': query,
            'crawled_at': datetime.now().isoformat(),
            'total_sources': len(sources),
            'successful_sources': self.stats['successful_jobs'],
            'failed_sources': self.stats['failed_jobs'],
            'mock_data_filtered': self.stats['mock_data_filtered'],
            'total_articles': self.stats['total_articles'],
            'sources': {},
            'errors': []
        }
        
        # ê²°ê³¼ ì •ë¦¬
        for result in results:
            if isinstance(result, Exception):
                integrated_results['errors'].append(str(result))
            elif isinstance(result, dict):
                if 'error' not in result:
                    # ì •ìƒ ê²°ê³¼
                    job_metadata = result.get('job_metadata', {})
                    source_name = job_metadata.get('source_name', 'unknown')
                    integrated_results['sources'][source_name] = result
                else:
                    # ì—ëŸ¬ ê²°ê³¼
                    integrated_results['errors'].append(result)
        
        # í ì´ˆê¸°í™”
        self.job_queue.clear()
        
        return integrated_results
    
    def get_statistics(self) -> Dict:
        """
        í¬ë¡¤ë§ í†µê³„ ë°˜í™˜
        """
        return {
            **self.stats,
            'cache_size': len(self.result_cache),
            'queue_size': len(self.job_queue),
            'success_rate': (
                self.stats['successful_jobs'] / max(self.stats['total_jobs'], 1) * 100
            ),
            'timestamp': datetime.now().isoformat()
        }
    
    def cleanup(self):
        """
        ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        """
        # í¬ë¡¤ëŸ¬ ì •ë¦¬
        for crawler_type, crawler in self.crawlers.items():
            if crawler and hasattr(crawler, 'cleanup'):
                crawler.cleanup()
        
        # ìºì‹œ ì •ë¦¬
        self.result_cache.clear()
        
        # í ì •ë¦¬
        self.job_queue.clear()
        
        logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
async def test_integrated_crawler():
    """
    í†µí•© í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    """
    manager = IntegratedCrawlerManager()
    
    try:
        # ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§
        results = await manager.crawl_all_sources("êµ­ë¯¼ì—°ê¸ˆ")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š í†µí•© í¬ë¡¤ë§ ê²°ê³¼")
        print("=" * 60)
        
        print(f"âœ… ì„±ê³µ: {results['successful_sources']}/{results['total_sources']}")
        print(f"âŒ ì‹¤íŒ¨: {results['failed_sources']}")
        print(f"ğŸš« Mock ë°ì´í„° í•„í„°ë§: {results['mock_data_filtered']}")
        print(f"ğŸ“„ ì´ ìˆ˜ì§‘ í•­ëª©: {results['total_articles']}ê°œ")
        
        print("\nğŸ“° ì†ŒìŠ¤ë³„ ê²°ê³¼:")
        for source_name, source_data in results['sources'].items():
            if 'job_metadata' in source_data:
                metadata = source_data['job_metadata']
                print(f"  â€¢ {source_name}: {metadata.get('article_count', 0)}ê°œ í•­ëª©")
        
        # í†µê³„ ì¶œë ¥
        stats = manager.get_statistics()
        print(f"\nğŸ“ˆ ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
        
        # JSON ì €ì¥
        output_file = f"integrated_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        
    finally:
        manager.cleanup()


if __name__ == "__main__":
    asyncio.run(test_integrated_crawler())
