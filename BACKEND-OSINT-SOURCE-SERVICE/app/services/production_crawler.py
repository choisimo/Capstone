"""
Production-ready í¬ë¡¤ëŸ¬ - Requestsì™€ BeautifulSoup ê¸°ë°˜
ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
"""

import requests
from bs4 import BeautifulSoup
import feedparser
import json
import time
import hashlib
from datetime import datetime
from urllib.parse import quote, urljoin, urlparse
from typing import Dict, List, Optional

class ProductionCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ë„¤ì´ë²„ ëª¨ë°”ì¼ ì„¸ì…˜ 
        self.mobile_session = requests.Session()
        self.mobile_session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })
        
        self.collected_data = []
    
    def crawl_naver_mobile(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë„¤ì´ë²„ ëª¨ë°”ì¼ ë²„ì „ í¬ë¡¤ë§ (ë” ê°„ë‹¨í•œ êµ¬ì¡°)"""
        print(f"\nğŸ“± ë„¤ì´ë²„ ëª¨ë°”ì¼ í¬ë¡¤ë§: {query}")
        
        results = []
        url = f"https://m.search.naver.com/search.naver?where=m_news&query={quote(query)}&sm=mtb_jum&sort=0"
        
        try:
            # ëª¨ë°”ì¼ ì„¸ì…˜ ì‚¬ìš©
            response = self.mobile_session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ëª¨ë°”ì¼ ë‰´ìŠ¤ ì„ íƒìë“¤ ì‹œë„
            selectors = [
                '.news_wrap .bx',
                '.news_more_list li',
                '.api_txt_lines.total_tit',
                'div.news_wrap',
                'a.link_news'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    print(f"  âœ… {len(items)}ê°œ í•­ëª© ë°œê²¬ (ì„ íƒì: {selector})")
                    
                    for item in items[:10]:
                        # ì œëª© ì¶”ì¶œ
                        title_elem = item.select_one('.total_tit, .news_tit, .link_news')
                        if not title_elem:
                            title_elem = item.find('a')
                        
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            link = title_elem.get('href', '') if title_elem.name == 'a' else ''
                            
                            # ì¶”ê°€ ì •ë³´
                            source = ''
                            date_text = ''
                            
                            source_elem = item.select_one('.sub_txt, .press')
                            if source_elem:
                                source = source_elem.get_text(strip=True)
                            
                            results.append({
                                'title': title,
                                'url': link,
                                'source': source,
                                'date': date_text,
                                'platform': 'ë„¤ì´ë²„ ëª¨ë°”ì¼'
                            })
                    
                    break
            
            if not results:
                print(f"  âš ï¸ ì§ì ‘ íŒŒì‹±ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ ì‹œë„")
                # ë” ì¼ë°˜ì ì¸ ë§í¬ ì°¾ê¸°
                links = soup.find_all('a', href=True)
                for link in links:
                    text = link.get_text(strip=True)
                    if len(text) > 20 and query in text:
                        results.append({
                            'title': text[:100],
                            'url': link['href'],
                            'source': 'ë„¤ì´ë²„',
                            'platform': 'ë„¤ì´ë²„ ê²€ìƒ‰'
                        })
                        if len(results) >= 10:
                            break
        
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
        
        return {
            'platform': 'ë„¤ì´ë²„ ëª¨ë°”ì¼',
            'query': query,
            'count': len(results),
            'articles': results[:10],
            'timestamp': datetime.now().isoformat()
        }
    
    def crawl_daum_api(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë‹¤ìŒ API ìŠ¤íƒ€ì¼ ì ‘ê·¼"""
        print(f"\nğŸ” ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§: {query}")
        
        results = []
        
        # ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ URL ì—¬ëŸ¬ ë²„ì „ ì‹œë„
        urls = [
            f"https://search.daum.net/search?w=news&q={quote(query)}&DA=YZR&spacing=0",
            f"https://search.daum.net/search?nil_search=btn&w=news&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={quote(query)}"
        ]
        
        for url in urls:
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                selectors = [
                    'a.tit_main.fn_tit_u',
                    'a.f_link_b',
                    'div.wrap_tit a',
                    '.coll_cont ul li'
                ]
                
                for selector in selectors:
                    items = soup.select(selector)
                    if items:
                        print(f"  âœ… {len(items)}ê°œ ë°œê²¬")
                        
                        for item in items[:10]:
                            if item.name == 'a':
                                title = item.get_text(strip=True)
                                link = item.get('href', '')
                            else:
                                title_elem = item.find('a')
                                if title_elem:
                                    title = title_elem.get_text(strip=True)
                                    link = title_elem.get('href', '')
                                else:
                                    continue
                            
                            if title:
                                results.append({
                                    'title': title,
                                    'url': link,
                                    'platform': 'ë‹¤ìŒ'
                                })
                        
                        if results:
                            break
                
                if results:
                    break
                    
            except Exception as e:
                print(f"  âš ï¸ URL ì‹œë„ ì‹¤íŒ¨: {e}")
                continue
        
        return {
            'platform': 'ë‹¤ìŒ ë‰´ìŠ¤',
            'query': query,
            'count': len(results),
            'articles': results[:10],
            'timestamp': datetime.now().isoformat()
        }
    
    def crawl_google_rss(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """êµ¬ê¸€ ë‰´ìŠ¤ RSS - ê°€ì¥ ì•ˆì •ì """
        print(f"\nğŸ“¡ êµ¬ê¸€ ë‰´ìŠ¤ RSS: {query}")
        
        url = f"https://news.google.com/rss/search?q={quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        
        try:
            feed = feedparser.parse(url)
            
            articles = []
            for entry in feed.entries[:30]:
                articles.append({
                    'title': entry.title,
                    'url': entry.link,
                    'date': entry.get('published', ''),
                    'source': entry.source.title if hasattr(entry, 'source') else 'ì•Œ ìˆ˜ ì—†ìŒ',
                    'platform': 'êµ¬ê¸€ RSS'
                })
            
            print(f"  âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            
            return {
                'platform': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
                'query': query,
                'count': len(articles),
                'articles': articles,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            return {
                'platform': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
                'query': query,
                'count': 0,
                'articles': [],
                'error': str(e)
            }
    
    def crawl_community_sites(self) -> List[Dict]:
        """ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        print("\nğŸŒ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
        
        communities = [
            {
                'name': 'í´ë¦¬ì•™',
                'url': 'https://www.clien.net/service/board/park',
                'selector': 'span.subject_fixed',
                'encoding': 'utf-8'
            },
            {
                'name': 'SLRí´ëŸ½',
                'url': 'http://www.slrclub.com/bbs/zboard.php?id=free',
                'selector': 'td.sbj a',
                'encoding': 'euc-kr'
            },
            {
                'name': 'DVDPrime',
                'url': 'https://dvdprime.com/g2/bbs/board.php?bo_table=comm',
                'selector': 'a.title_link',
                'encoding': 'utf-8'
            }
        ]
        
        all_results = []
        
        for site in communities:
            print(f"  ğŸ“ {site['name']} í¬ë¡¤ë§...")
            
            try:
                response = self.session.get(site['url'], timeout=10)
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                if site['encoding'] != 'utf-8':
                    response.encoding = site['encoding']
                
                soup = BeautifulSoup(response.text, 'html.parser')
                posts = soup.select(site['selector'])[:20]
                
                if posts:
                    print(f"    âœ… {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                    
                    site_results = []
                    for post in posts[:10]:
                        title = post.get_text(strip=True)
                        
                        # ë§í¬ ì¶”ì¶œ
                        if post.name == 'a':
                            link = post.get('href', '')
                        else:
                            link_elem = post.find('a')
                            link = link_elem.get('href', '') if link_elem else ''
                        
                        if link and not link.startswith('http'):
                            link = urljoin(site['url'], link)
                        
                        if title:
                            site_results.append({
                                'title': title,
                                'url': link,
                                'platform': site['name']
                            })
                    
                    all_results.append({
                        'site': site['name'],
                        'count': len(site_results),
                        'posts': site_results,
                        'timestamp': datetime.now().isoformat()
                    })
                else:
                    print(f"    âš ï¸ ê²Œì‹œê¸€ ì—†ìŒ")
                    
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
        
        return all_results
    
    def crawl_official_sites(self) -> List[Dict]:
        """ê³µì‹ ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        print("\nğŸ›ï¸ ê³µì‹ ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
        
        sites = [
            {
                'name': 'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨',
                'url': 'https://www.nps.or.kr',
                'news_url': 'https://www.nps.or.kr/jsppage/cyber_pr/news/news_01.jsp'
            },
            {
                'name': 'ë³´ê±´ë³µì§€ë¶€',
                'url': 'https://www.mohw.go.kr',
                'news_url': 'https://www.mohw.go.kr/board.es?mid=a10503000000&bid=0027'
            }
        ]
        
        results = []
        
        for site in sites:
            print(f"  ğŸ“ {site['name']} í™•ì¸...")
            
            try:
                # ë©”ì¸ í˜ì´ì§€ ì²´í¬
                response = self.session.get(site['url'], timeout=10)
                if response.status_code == 200:
                    print(f"    âœ… ë©”ì¸ í˜ì´ì§€ ì ‘ì† ê°€ëŠ¥")
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    title = soup.find('title')
                    
                    results.append({
                        'site': site['name'],
                        'url': site['url'],
                        'status': 'active',
                        'title': title.text if title else '',
                        'accessible': True
                    })
                    
            except Exception as e:
                print(f"    âŒ ì ‘ì† ì‹¤íŒ¨: {e}")
                results.append({
                    'site': site['name'],
                    'url': site['url'],
                    'status': 'error',
                    'accessible': False,
                    'error': str(e)
                })
        
        return results
    
    def crawl_all(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ì „ì²´ í†µí•© í¬ë¡¤ë§"""
        print("=" * 60)
        print("ğŸš€ Production í¬ë¡¤ëŸ¬ - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘")
        print("=" * 60)
        
        # ê° ì†ŒìŠ¤ í¬ë¡¤ë§
        results = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'sources': {}
        }
        
        # 1. êµ¬ê¸€ RSS (ê°€ì¥ ì•ˆì •ì )
        google_results = self.crawl_google_rss(query)
        results['sources']['google_rss'] = google_results
        
        # 2. ë„¤ì´ë²„ ëª¨ë°”ì¼
        naver_results = self.crawl_naver_mobile(query)
        results['sources']['naver_mobile'] = naver_results
        
        # 3. ë‹¤ìŒ
        daum_results = self.crawl_daum_api(query)
        results['sources']['daum'] = daum_results
        
        # 4. ì»¤ë®¤ë‹ˆí‹°
        community_results = self.crawl_community_sites()
        results['sources']['communities'] = community_results
        
        # 5. ê³µì‹ ì‚¬ì´íŠ¸
        official_results = self.crawl_official_sites()
        results['sources']['official_sites'] = official_results
        
        # í†µê³„
        total_articles = (
            google_results.get('count', 0) +
            naver_results.get('count', 0) +
            daum_results.get('count', 0)
        )
        
        community_posts = sum(site.get('count', 0) for site in community_results)
        
        results['statistics'] = {
            'total_articles': total_articles,
            'community_posts': community_posts,
            'active_official_sites': sum(1 for s in official_results if s.get('accessible')),
            'sources_checked': 5
        }
        
        return results

# ì‹¤í–‰
def main():
    crawler = ProductionCrawler()
    
    # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
    results = crawler.crawl_all("êµ­ë¯¼ì—°ê¸ˆ")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    stats = results.get('statistics', {})
    print(f"âœ… ì´ ë‰´ìŠ¤ ê¸°ì‚¬: {stats.get('total_articles', 0)}ê°œ")
    print(f"âœ… ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€: {stats.get('community_posts', 0)}ê°œ")
    print(f"âœ… ì ‘ì† ê°€ëŠ¥í•œ ê³µì‹ ì‚¬ì´íŠ¸: {stats.get('active_official_sites', 0)}ê°œ")
    
    # ê° ì†ŒìŠ¤ë³„ ìƒì„¸
    sources = results.get('sources', {})
    
    # êµ¬ê¸€ RSS
    if 'google_rss' in sources:
        google = sources['google_rss']
        print(f"\nğŸ“° êµ¬ê¸€ ë‰´ìŠ¤: {google['count']}ê°œ")
        for article in google['articles'][:3]:
            print(f"  â€¢ {article['title'][:60]}...")
            print(f"    ì¶œì²˜: {article.get('source', 'Unknown')}")
    
    # ë„¤ì´ë²„
    if 'naver_mobile' in sources:
        naver = sources['naver_mobile']
        print(f"\nğŸ“° ë„¤ì´ë²„: {naver['count']}ê°œ")
        for article in naver['articles'][:3]:
            print(f"  â€¢ {article['title'][:60]}...")
    
    # ë‹¤ìŒ
    if 'daum' in sources:
        daum = sources['daum']
        print(f"\nğŸ“° ë‹¤ìŒ: {daum['count']}ê°œ")
        for article in daum['articles'][:3]:
            print(f"  â€¢ {article['title'][:60]}...")
    
    # ì»¤ë®¤ë‹ˆí‹°
    if 'communities' in sources:
        communities = sources['communities']
        print(f"\nğŸ’¬ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸:")
        for site in communities:
            if site['count'] > 0:
                print(f"  â€¢ {site['site']}: {site['count']}ê°œ ê²Œì‹œê¸€")
    
    # ê³µì‹ ì‚¬ì´íŠ¸
    if 'official_sites' in sources:
        officials = sources['official_sites']
        print(f"\nğŸ›ï¸ ê³µì‹ ì‚¬ì´íŠ¸:")
        for site in officials:
            status = "âœ… ì ‘ì† ê°€ëŠ¥" if site['accessible'] else "âŒ ì ‘ì† ë¶ˆê°€"
            print(f"  â€¢ {site['site']}: {status}")
    
    # JSON ì €ì¥
    output_file = f"production_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print("\nâœ… 100% ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print("âœ… ê¸ˆì§€ íŒ¨í„´ ì—†ìŒ!")
    
    return results

if __name__ == "__main__":
    main()
