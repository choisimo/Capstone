"""
Ultimate í¬ë¡¤ëŸ¬ - Seleniumê³¼ aiohttpë¥¼ ì¡°í•©í•œ ì™„ë²½í•œ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
import feedparser
import json
from datetime import datetime
from urllib.parse import quote, urljoin
from typing import Dict, List, Optional
import time

# Selenium ê´€ë ¨
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

class UltimateCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.driver = None
    
    def setup_selenium_driver(self):
        """Selenium ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
        options = Options()
        options.add_argument('--headless')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.add_argument('--lang=ko-KR,ko;q=0.9')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            # ë´‡ ê°ì§€ ìš°íšŒ
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            return True
        except Exception as e:
            print(f"âŒ Selenium ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def crawl_with_selenium(self, url: str, wait_selector: str = None, wait_time: int = 10) -> Optional[str]:
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            if not self.driver:
                if not self.setup_selenium_driver():
                    return None
            
            self.driver.get(url)
            
            # íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            if wait_selector:
                try:
                    WebDriverWait(self.driver, wait_time).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))
                    )
                except TimeoutException:
                    print(f"âš ï¸ ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {wait_selector}")
            else:
                time.sleep(2)  # ê¸°ë³¸ ëŒ€ê¸°
            
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ë™ì  ì»¨í…ì¸  ë¡œë“œ)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(1)
            
            return self.driver.page_source
        except Exception as e:
            print(f"âŒ Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    async def fetch_with_aiohttp(self, url: str) -> Optional[str]:
        """aiohttpë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° í¬ë¡¤ë§ (Brotli ì§€ì›)"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url, headers=self.headers) as response:
                    if response.status == 200:
                        return await response.text()
        except Exception as e:
            print(f"âŒ aiohttp í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    async def crawl_naver_news(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - Selenium ì‚¬ìš©"""
        print(f"\nğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ (Selenium): {query}")
        
        url = f"https://search.naver.com/search.naver?where=news&query={quote(query)}"
        html = await asyncio.to_thread(self.crawl_with_selenium, url, wait_selector="a.news_tit", wait_time=5)
        
        results = []
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            news_items = soup.select('a.news_tit')
            
            print(f"  âœ… {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            for item in news_items[:10]:
                title = item.get_text(strip=True)
                link = item.get('href', '')
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                parent = item.find_parent('div', class_='news_area')
                source = ''
                date_text = ''
                
                if parent:
                    source_elem = parent.select_one('.info_group .press')
                    if source_elem:
                        source = source_elem.get_text(strip=True).replace('ì–¸ë¡ ì‚¬ ì„ ì •', '').strip()
                    
                    date_elem = parent.select_one('.info_group span.info')
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                
                results.append({
                    'title': title,
                    'url': link,
                    'source': source,
                    'date': date_text,
                    'platform': 'ë„¤ì´ë²„'
                })
        
        return {
            'platform': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
            'query': query,
            'count': len(results),
            'articles': results,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_daum_news(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ - Selenium ì‚¬ìš©"""
        print(f"\nğŸ” ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ (Selenium): {query}")
        
        url = f"https://search.daum.net/search?w=news&q={quote(query)}"
        html = await asyncio.to_thread(self.crawl_with_selenium, url, wait_selector="a.tit_main", wait_time=5)
        
        results = []
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            news_items = soup.select('a.tit_main')
            
            print(f"  âœ… {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            for item in news_items[:10]:
                title = item.get_text(strip=True)
                link = item.get('href', '')
                
                # ì¶”ê°€ ì •ë³´
                parent = item.find_parent('div', class_='item-bundled')
                source = ''
                date_text = ''
                
                if parent:
                    source_elem = parent.select_one('.f_nb')
                    if source_elem:
                        source = source_elem.get_text(strip=True)
                    
                    date_elem = parent.select_one('.f_nb.date')
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                
                results.append({
                    'title': title,
                    'url': link,
                    'source': source,
                    'date': date_text,
                    'platform': 'ë‹¤ìŒ'
                })
        
        return {
            'platform': 'ë‹¤ìŒ ë‰´ìŠ¤',
            'query': query,
            'count': len(results),
            'articles': results,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_community_sites_selenium(self) -> List[Dict]:
        """ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ - Selenium"""
        print("\nğŸŒ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (Selenium)")
        
        communities = [
            {
                'name': 'í´ë¦¬ì•™',
                'url': 'https://www.clien.net/service/board/park',
                'selector': 'span.subject_fixed',
                'wait_selector': 'div.list_item'
            },
            {
                'name': 'ë½ë¿Œ',
                'url': 'https://www.ppomppu.co.kr/zboard/zboard.php?id=freeboard',
                'selector': 'font.list_title',
                'wait_selector': 'table.board_table'
            },
            {
                'name': 'ë³´ë°°ë“œë¦¼',
                'url': 'https://www.bobaedream.co.kr/list?code=freeb',
                'selector': 'a.bsubject',
                'wait_selector': 'tbody.list'
            },
            {
                'name': 'FMì½”ë¦¬ì•„',
                'url': 'https://www.fmkorea.com/best',
                'selector': 'h3.title a',
                'wait_selector': 'ul.fm_best_widget'
            }
        ]
        
        all_results = []
        
        for site in communities:
            print(f"  ğŸ“ {site['name']} í¬ë¡¤ë§ ì¤‘...")
            
            html = await asyncio.to_thread(self.crawl_with_selenium, site['url'], wait_selector=site.get('wait_selector'), wait_time=5)
            
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                posts = soup.select(site['selector'])[:20]
                
                if posts:
                    print(f"    âœ… {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                    
                    site_results = []
                    for post in posts[:10]:
                        title = post.get_text(strip=True)
                        link = ''
                        
                        if post.name == 'a' and post.get('href'):
                            link = urljoin(site['url'], post.get('href'))
                        elif post.find_parent('a'):
                            parent_link = post.find_parent('a')
                            if parent_link and parent_link.get('href'):
                                link = urljoin(site['url'], parent_link.get('href'))
                        
                        if title:
                            site_results.append({
                                'title': title,
                                'url': link,
                                'platform': site['name']
                            })
                    
                    all_results.append({
                        'site': site['name'],
                        'count': len(site_results),
                        'posts': site_results,
                        'timestamp': datetime.now().isoformat()
                    })
                else:
                    print(f"    âš ï¸ ê²Œì‹œê¸€ ì—†ìŒ")
        
        return all_results
    
    async def crawl_google_rss(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """êµ¬ê¸€ ë‰´ìŠ¤ RSS í¬ë¡¤ë§"""
        print(f"\nğŸ“¡ êµ¬ê¸€ ë‰´ìŠ¤ RSS í¬ë¡¤ë§: {query}")
        
        url = f"https://news.google.com/rss/search?q={quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(url)
        
        articles = []
        for entry in feed.entries[:20]:
            articles.append({
                'title': entry.title,
                'url': entry.link,
                'date': entry.get('published', ''),
                'source': entry.source.title if hasattr(entry, 'source') else '',
                'platform': 'êµ¬ê¸€ RSS'
            })
        
        print(f"  âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        
        return {
            'platform': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
            'query': query,
            'count': len(articles),
            'articles': articles,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_all(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ëª¨ë“  ì†ŒìŠ¤ í†µí•© í¬ë¡¤ë§"""
        print("=" * 60)
        print("ğŸš€ Ultimate í¬ë¡¤ëŸ¬ - ì „ì²´ ì†ŒìŠ¤ ìˆ˜ì§‘")
        print("=" * 60)
        
        # Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        if not self.setup_selenium_driver():
            print("âš ï¸ Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨, ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ")
        
        # ë¹„ë™ê¸° ì‘ì—…ë“¤
        tasks = [
            self.crawl_naver_news(query),
            self.crawl_daum_news(query),
            self.crawl_google_rss(query),
            self.crawl_community_sites_selenium()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        if self.driver:
            self.driver.quit()
        
        # ê²°ê³¼ ì •ë¦¬
        total_count = 0
        final_results = {
            'status': 'success',
            'query': query,
            'sources': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # ê° ì†ŒìŠ¤ë³„ ê²°ê³¼ ì²˜ë¦¬
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                continue
            
            if i == 0:  # ë„¤ì´ë²„
                final_results['sources']['naver'] = result
                total_count += result.get('count', 0)
            elif i == 1:  # ë‹¤ìŒ
                final_results['sources']['daum'] = result
                total_count += result.get('count', 0)
            elif i == 2:  # êµ¬ê¸€ RSS
                final_results['sources']['google_rss'] = result
                total_count += result.get('count', 0)
            elif i == 3:  # ì»¤ë®¤ë‹ˆí‹°
                final_results['sources']['communities'] = result
                if isinstance(result, list):
                    for site in result:
                        total_count += site.get('count', 0)
        
        final_results['total_articles'] = total_count
        
        return final_results
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
async def test_ultimate_crawler():
    crawler = UltimateCrawler()
    
    try:
        # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawler.crawl_all("êµ­ë¯¼ì—°ê¸ˆ")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        print(f"âœ… ì´ ìˆ˜ì§‘ í•­ëª©: {results['total_articles']}ê°œ")
        
        # ê° ì†ŒìŠ¤ë³„ ê²°ê³¼
        sources = results.get('sources', {})
        
        if 'naver' in sources:
            print(f"  - ë„¤ì´ë²„: {sources['naver']['count']}ê°œ")
            for article in sources['naver']['articles'][:3]:
                print(f"    â€¢ {article['title'][:50]}...")
        
        if 'daum' in sources:
            print(f"  - ë‹¤ìŒ: {sources['daum']['count']}ê°œ")
            for article in sources['daum']['articles'][:3]:
                print(f"    â€¢ {article['title'][:50]}...")
        
        if 'google_rss' in sources:
            print(f"  - êµ¬ê¸€ RSS: {sources['google_rss']['count']}ê°œ")
            for article in sources['google_rss']['articles'][:3]:
                print(f"    â€¢ {article['title'][:50]}...")
        
        if 'communities' in sources:
            communities = sources['communities']
            if communities:
                print(f"  - ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸:")
                for site in communities:
                    if site['count'] > 0:
                        print(f"    â€¢ {site['site']}: {site['count']}ê°œ")
        
        # JSON ì €ì¥
        output_file = f"ultimate_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        print("\nâœ… ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ê¸ˆì§€ íŒ¨í„´ ì—†ìŒ!")
        
    finally:
        crawler.cleanup()

if __name__ == "__main__":
    asyncio.run(test_ultimate_crawler())
