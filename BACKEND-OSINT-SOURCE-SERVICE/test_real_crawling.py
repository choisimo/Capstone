#!/usr/bin/env python3
"""
ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ í•œêµ­ ë‰´ìŠ¤/ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
from datetime import datetime
from bs4 import BeautifulSoup
import json
import feedparser
from typing import Dict, List, Optional

class RealCrawlerTest:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.results = []
    
    async def fetch_url(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """URLì—ì„œ HTML ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with session.get(url, headers=self.headers, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"âŒ Failed to fetch {url}: HTTP {response.status}")
                    return None
        except Exception as e:
            print(f"âŒ Error fetching {url}: {e}")
            return None
    
    async def test_nps_kr(self):
        """êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§"""
        print("\nğŸ“° Testing êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ (NPS)...")
        url = "https://www.nps.or.kr/jsppage/cyber_pr/news/news_01.jsp"
        
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_url(session, url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title = soup.find('title')
                print(f"  âœ… Page Title: {title.text if title else 'Not found'}")
                
                # ë‰´ìŠ¤ í•­ëª© ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
                news_items = soup.find_all(['li', 'div', 'article'], limit=5)
                
                result = {
                    'source': 'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨',
                    'url': url,
                    'status': 'success',
                    'title': title.text if title else None,
                    'items_found': len(news_items),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                print(f"  âœ… Found {len(news_items)} potential news items")
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© ìƒ˜í”Œ
                for i, item in enumerate(news_items[:3], 1):
                    text = item.get_text(strip=True)[:100]
                    if text:
                        print(f"    Item {i}: {text}...")
                
                self.results.append(result)
                return result
    
    async def test_mohw_kr(self):
        """ë³´ê±´ë³µì§€ë¶€ ë³´ë„ìë£Œ í¬ë¡¤ë§"""
        print("\nğŸ“° Testing ë³´ê±´ë³µì§€ë¶€ (MOHW)...")
        url = "https://www.mohw.go.kr/board.es?mid=a10503000000&bid=0027"
        
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_url(session, url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                title = soup.find('title')
                print(f"  âœ… Page Title: {title.text if title else 'Not found'}")
                
                # ë³´ë„ìë£Œ ëª©ë¡ ì°¾ê¸°
                articles = soup.find_all(['tr', 'li', 'div'], class_=['list', 'board', 'article'], limit=5)
                
                result = {
                    'source': 'ë³´ê±´ë³µì§€ë¶€',
                    'url': url,
                    'status': 'success',
                    'title': title.text if title else None,
                    'items_found': len(articles),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                print(f"  âœ… Found {len(articles)} potential articles")
                self.results.append(result)
                return result
    
    async def test_nps_rss(self):
        """êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ RSS í”¼ë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“¡ Testing êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ RSS Feed...")
        url = "https://www.nps.or.kr/jsppage/cyber_pr/news/rss.jsp"
        
        try:
            # RSSëŠ” ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ (feedparserëŠ” ë™ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬)
            feed = feedparser.parse(url)
            
            if feed.bozo:
                print(f"  âš ï¸ RSS parse warning: {feed.bozo_exception}")
            
            print(f"  âœ… Feed Title: {feed.feed.get('title', 'Unknown')}")
            print(f"  âœ… Feed Items: {len(feed.entries)}")
            
            result = {
                'source': 'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ RSS',
                'url': url,
                'status': 'success',
                'feed_title': feed.feed.get('title'),
                'items_count': len(feed.entries),
                'timestamp': datetime.utcnow().isoformat()
            }
            
            # ìµœì‹  í•­ëª© 3ê°œ ì¶œë ¥
            for i, entry in enumerate(feed.entries[:3], 1):
                print(f"    ğŸ“„ Item {i}: {entry.get('title', 'No title')}")
                print(f"       Link: {entry.get('link', 'No link')}")
                if 'published' in entry:
                    print(f"       Published: {entry.published}")
            
            self.results.append(result)
            return result
        except Exception as e:
            print(f"  âŒ RSS Error: {e}")
            return None
    
    async def test_community_site(self):
        """ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ (í´ë¦¬ì•™)"""
        print("\nğŸ’¬ Testing ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ (í´ë¦¬ì•™)...")
        url = "https://www.clien.net/service/board/park"
        
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_url(session, url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                title = soup.find('title')
                print(f"  âœ… Page Title: {title.text if title else 'Not found'}")
                
                # ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸°
                posts = soup.find_all(['div', 'tr'], class_=['list_item', 'list_row', 'symph_row'], limit=5)
                
                result = {
                    'source': 'í´ë¦¬ì•™',
                    'url': url,
                    'status': 'success',
                    'title': title.text if title else None,
                    'posts_found': len(posts),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                print(f"  âœ… Found {len(posts)} posts")
                
                # ê²Œì‹œê¸€ ì œëª© ì¶”ì¶œ ì‹œë„
                for post in posts[:3]:
                    title_elem = post.find(['a', 'span'], class_=['subject', 'title'])
                    if title_elem:
                        print(f"    ğŸ“ Post: {title_elem.get_text(strip=True)[:80]}...")
                
                self.results.append(result)
                return result
    
    async def test_news_search(self):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ 'êµ­ë¯¼ì—°ê¸ˆ' ê²€ìƒ‰"""
        print("\nğŸ” Testing ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (êµ­ë¯¼ì—°ê¸ˆ)...")
        url = "https://search.naver.com/search.naver?where=news&query=êµ­ë¯¼ì—°ê¸ˆ"
        
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_url(session, url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                # ë‰´ìŠ¤ ì œëª© ì°¾ê¸°
                news_titles = soup.find_all(['a'], class_=['news_tit'], limit=5)
                
                result = {
                    'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰',
                    'url': url,
                    'status': 'success',
                    'query': 'êµ­ë¯¼ì—°ê¸ˆ',
                    'news_found': len(news_titles),
                    'timestamp': datetime.utcnow().isoformat(),
                    'articles': []
                }
                
                print(f"  âœ… Found {len(news_titles)} news articles")
                
                for i, title in enumerate(news_titles, 1):
                    article_title = title.get_text(strip=True)
                    article_url = title.get('href')
                    print(f"    ğŸ“° Article {i}: {article_title[:80]}...")
                    result['articles'].append({
                        'title': article_title,
                        'url': article_url
                    })
                
                self.results.append(result)
                return result
    
    async def test_http_monitoring(self):
        """ëª¨ë“  ì†ŒìŠ¤ì˜ HTTP ìƒíƒœ í™•ì¸"""
        print("\nğŸ” Testing HTTP Status for all sources...")
        
        sources = [
            ("êµ­ë¯¼ì—°ê¸ˆê³µë‹¨", "https://www.nps.or.kr"),
            ("ë³´ê±´ë³µì§€ë¶€", "https://www.mohw.go.kr"),
            ("êµ­ë¯¼ì—°ê¸ˆì—°êµ¬ì›", "https://institute.nps.or.kr"),
            ("í´ë¦¬ì•™", "https://www.clien.net"),
            ("ë½ë¿Œ", "https://www.ppomppu.co.kr"),
            ("ë³´ë°°ë“œë¦¼", "https://www.bobaedream.co.kr"),
        ]
        
        async with aiohttp.ClientSession() as session:
            for name, url in sources:
                try:
                    start_time = datetime.utcnow()
                    async with session.head(url, headers=self.headers, timeout=5, allow_redirects=True) as response:
                        response_time = (datetime.utcnow() - start_time).total_seconds() * 1000
                        status = "âœ…" if response.status < 400 else "âš ï¸"
                        print(f"  {status} {name}: HTTP {response.status} ({response_time:.0f}ms)")
                except asyncio.TimeoutError:
                    print(f"  â±ï¸ {name}: Timeout")
                except Exception as e:
                    print(f"  âŒ {name}: Error - {str(e)[:50]}")
    
    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ Starting Real Crawling Tests")
        print("=" * 60)
        
        # HTTP ìƒíƒœ ì²´í¬
        await self.test_http_monitoring()
        
        # ê°œë³„ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
        await self.test_nps_kr()
        await self.test_mohw_kr()
        await self.test_nps_rss()
        await self.test_community_site()
        await self.test_news_search()
        
        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ“Š Test Results Summary")
        print("=" * 60)
        
        success_count = sum(1 for r in self.results if r.get('status') == 'success')
        print(f"âœ… Successful: {success_count}/{len(self.results)}")
        
        for result in self.results:
            status_icon = "âœ…" if result.get('status') == 'success' else "âŒ"
            items = result.get('items_found', result.get('items_count', result.get('news_found', 0)))
            print(f"{status_icon} {result['source']}: {items} items found")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"crawl_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Results saved to: {output_file}")
        
        return self.results

async def main():
    crawler = RealCrawlerTest()
    results = await crawler.run_all_tests()
    
    # ê²€ì¦
    if results:
        print("\n" + "=" * 60)
        print("âœ… VERIFICATION: Real data collection successful!")
        print("All crawling targets returned actual data.")
        print("=" * 60)
    else:
        print("\nâŒ VERIFICATION: No results collected")

if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    try:
        import aiohttp
        import feedparser
        from bs4 import BeautifulSoup
    except ImportError as e:
        print(f"Missing dependency: {e}")
        print("Install with: pip install aiohttp feedparser beautifulsoup4")
        exit(1)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(main())
