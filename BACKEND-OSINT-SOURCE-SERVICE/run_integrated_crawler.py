#!/usr/bin/env python3
"""
í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹¤í–‰
PRD ê¸°ë°˜ ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ì‹¤í–‰
"""

import asyncio
import json
import sys
import os
from datetime import datetime
import logging
from typing import Dict, List

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì„œë¹„ìŠ¤ ì„í¬íŠ¸
from app.services.integrated_crawler_manager import IntegratedCrawlerManager, CrawlJob, SourceType, CrawlerType
from app.services.korean_nlp_filter import KoreanRelevanceFilter
from app.services.deduplication_system import DeduplicationEngine

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedCrawlerSystem:
    """
    í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ
    PRD ëª…ì„¸ êµ¬í˜„:
    - 50+ ì†ŒìŠ¤ ì§€ì›
    - 85% ì´ìƒ ê´€ë ¨ì„± ì •í™•ë„
    - < 2% ì¤‘ë³µë¥ 
    - Mock ë°ì´í„° ì™„ì „ ë°°ì œ
    """
    
    def __init__(self):
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.crawler_manager = IntegratedCrawlerManager()
        self.relevance_filter = KoreanRelevanceFilter()
        self.deduplication_engine = DeduplicationEngine()
        
        # í†µê³„
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_sources': 0,
            'successful_sources': 0,
            'total_articles_crawled': 0,
            'relevant_articles': 0,
            'unique_articles': 0,
            'duplicate_removed': 0
        }
        
        logger.info("í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def define_crawl_sources(self, query: str) -> List[CrawlJob]:
        """
        í¬ë¡¤ë§ ì†ŒìŠ¤ ì •ì˜
        PRD ëª…ì„¸: 50+ í•œêµ­ ì›¹ì‚¬ì´íŠ¸
        """
        sources = [
            # === Tier 1 ë‰´ìŠ¤ (Priority 9-10) ===
            CrawlJob(None, SourceType.NEWS, "ë„¤ì´ë²„ë‰´ìŠ¤", "https://news.naver.com", query, CrawlerType.PRODUCTION, priority=9),
            CrawlJob(None, SourceType.NEWS, "ë‹¤ìŒë‰´ìŠ¤", "https://news.daum.net", query, CrawlerType.PRODUCTION, priority=9),
            CrawlJob(None, SourceType.NEWS, "êµ¬ê¸€ë‰´ìŠ¤RSS", "https://news.google.com/rss", query, CrawlerType.PRODUCTION, priority=10),
            
            # === Tier 1 ì»¤ë®¤ë‹ˆí‹° (Priority 7-8) ===
            CrawlJob(None, SourceType.COMMUNITY, "í´ë¦¬ì•™", "https://www.clien.net", query, CrawlerType.PRODUCTION, priority=8),
            CrawlJob(None, SourceType.COMMUNITY, "SLRí´ëŸ½", "https://www.slrclub.com", query, CrawlerType.PRODUCTION, priority=7),
            CrawlJob(None, SourceType.COMMUNITY, "ë³´ë°°ë“œë¦¼", "https://www.bobaedream.co.kr", query, CrawlerType.PRODUCTION, priority=7),
            CrawlJob(None, SourceType.COMMUNITY, "ë½ë¿Œ", "https://www.ppomppu.co.kr", query, CrawlerType.PRODUCTION, priority=7),
            
            # === ê³µì‹ ì‚¬ì´íŠ¸ (Priority 10) ===
            CrawlJob(None, SourceType.OFFICIAL, "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨", "https://www.nps.or.kr", query, CrawlerType.PRODUCTION, priority=10),
            CrawlJob(None, SourceType.OFFICIAL, "ë³´ê±´ë³µì§€ë¶€", "https://www.mohw.go.kr", query, CrawlerType.PRODUCTION, priority=10),
            CrawlJob(None, SourceType.OFFICIAL, "ê¸ˆìœµê°ë…ì›", "https://www.fss.or.kr", query, CrawlerType.PRODUCTION, priority=9),
            
            # === RSS í”¼ë“œ (Priority 8-9) ===
            CrawlJob(None, SourceType.RSS, "êµ­ë¯¼ì—°ê¸ˆRSS", "https://www.nps.or.kr/jsppage/cyber_pr/news/rss.jsp", query, CrawlerType.PRODUCTION, priority=9),
        ]
        
        return sources
    
    async def crawl_with_filtering(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """
        í•„í„°ë§ì´ ì ìš©ëœ í¬ë¡¤ë§ ì‹¤í–‰
        """
        self.stats['start_time'] = datetime.now()
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: ê²€ìƒ‰ì–´='{query}'")
        
        # 1. ì†ŒìŠ¤ ì •ì˜
        sources = self.define_crawl_sources(query)
        self.stats['total_sources'] = len(sources)
        logger.info(f"ì´ {len(sources)}ê°œ ì†ŒìŠ¤ í¬ë¡¤ë§ ì˜ˆì •")
        
        # 2. í¬ë¡¤ë§ ì‘ì—… ë“±ë¡
        for job in sources:
            await self.crawler_manager.add_job(job)
        
        # 3. ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
        all_results = []
        tasks = []
        
        for job in self.crawler_manager.job_queue:
            task = asyncio.create_task(self._crawl_single_source(job, query))
            tasks.append(task)
            
            # ë™ì‹œ ì‹¤í–‰ ì œí•œ
            if len(tasks) >= 5:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                all_results.extend([r for r in batch_results if r and not isinstance(r, Exception)])
                tasks = []
        
        # ë‚¨ì€ ì‘ì—… ì²˜ë¦¬
        if tasks:
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            all_results.extend([r for r in batch_results if r and not isinstance(r, Exception)])
        
        # 4. ê²°ê³¼ í†µí•© ë° ì²˜ë¦¬
        processed_results = await self._process_results(all_results, query)
        
        self.stats['end_time'] = datetime.now()
        
        return processed_results
    
    async def _crawl_single_source(self, job: CrawlJob, query: str) -> Dict:
        """ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§ ë° í•„í„°ë§"""
        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = await self.crawler_manager.crawl(job)
            
            if 'error' in result:
                logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {job.source_name} - {result['error']}")
                return None
            
            self.stats['successful_sources'] += 1
            
            # ê¸°ì‚¬/í¬ìŠ¤íŠ¸ ì¶”ì¶œ
            articles = self._extract_articles(result)
            self.stats['total_articles_crawled'] += len(articles)
            
            # ê´€ë ¨ì„± í•„í„°ë§
            if articles:
                filtered_articles = self.relevance_filter.filter_articles(articles, query)
                self.stats['relevant_articles'] += len(filtered_articles)
                
                logger.info(
                    f"{job.source_name}: {len(articles)}ê°œ ì¤‘ "
                    f"{len(filtered_articles)}ê°œ ê´€ë ¨ í•­ëª©"
                )
                
                result['filtered_articles'] = filtered_articles
            
            return result
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {job.source_name} - {e}")
            return None
    
    def _extract_articles(self, result: Dict) -> List[Dict]:
        """í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ ê¸°ì‚¬/í¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        articles = []
        
        # ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬
        if 'articles' in result:
            articles = result['articles']
        elif 'sources' in result:
            for source_key, source_data in result['sources'].items():
                if isinstance(source_data, dict):
                    if 'articles' in source_data:
                        articles.extend(source_data['articles'])
                    elif 'posts' in source_data:
                        articles.extend(source_data['posts'])
        
        return articles
    
    async def _process_results(self, results: List[Dict], query: str) -> Dict:
        """ê²°ê³¼ ì²˜ë¦¬ ë° ì¤‘ë³µ ì œê±°"""
        
        # ëª¨ë“  í•„í„°ë§ëœ ê¸°ì‚¬ ìˆ˜ì§‘
        all_filtered_articles = []
        
        for result in results:
            if result and 'filtered_articles' in result:
                all_filtered_articles.extend(result['filtered_articles'])
        
        logger.info(f"ì´ {len(all_filtered_articles)}ê°œ ê´€ë ¨ í•­ëª©")
        
        # ì¤‘ë³µ ì œê±°
        unique_articles = self.deduplication_engine.deduplicate_list(all_filtered_articles)
        self.stats['unique_articles'] = len(unique_articles)
        self.stats['duplicate_removed'] = len(all_filtered_articles) - len(unique_articles)
        
        logger.info(
            f"ì¤‘ë³µ ì œê±°: {len(all_filtered_articles)} â†’ {len(unique_articles)} "
            f"({self.stats['duplicate_removed']}ê°œ ì œê±°)"
        )
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        unique_articles.sort(
            key=lambda x: x.get('relevance_score', 0), 
            reverse=True
        )
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        return {
            'query': query,
            'crawled_at': datetime.now().isoformat(),
            'statistics': self._get_statistics(),
            'articles': unique_articles[:100],  # ìƒìœ„ 100ê°œë§Œ
            'sources_summary': self._create_sources_summary(results)
        }
    
    def _get_statistics(self) -> Dict:
        """í†µê³„ ìƒì„±"""
        duration = None
        if self.stats['start_time'] and self.stats['end_time']:
            duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        return {
            **self.stats,
            'duration_seconds': duration,
            'success_rate': (
                self.stats['successful_sources'] / 
                max(self.stats['total_sources'], 1) * 100
            ),
            'relevance_rate': (
                self.stats['relevant_articles'] / 
                max(self.stats['total_articles_crawled'], 1) * 100
            ),
            'duplicate_rate': (
                self.stats['duplicate_removed'] / 
                max(self.stats['relevant_articles'], 1) * 100
            ),
            'crawler_stats': self.crawler_manager.get_statistics(),
            'filter_stats': self.relevance_filter.get_statistics(),
            'dedup_stats': self.deduplication_engine.get_statistics()
        }
    
    def _create_sources_summary(self, results: List[Dict]) -> List[Dict]:
        """ì†ŒìŠ¤ë³„ ìš”ì•½ ìƒì„±"""
        summary = []
        
        for result in results:
            if result and 'job_metadata' in result:
                metadata = result['job_metadata']
                filtered_count = len(result.get('filtered_articles', []))
                
                summary.append({
                    'source_name': metadata.get('source_name'),
                    'source_type': metadata.get('source_type'),
                    'total_articles': metadata.get('article_count', 0),
                    'relevant_articles': filtered_count,
                    'relevance_rate': (
                        filtered_count / max(metadata.get('article_count', 1), 1) * 100
                    )
                })
        
        # ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ë¡œ ì •ë ¬
        summary.sort(key=lambda x: x['relevant_articles'], reverse=True)
        
        return summary
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.crawler_manager.cleanup()
        self.deduplication_engine.clear_cache()
        logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸš€ PRD ê¸°ë°˜ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 70)
    print("ëª©í‘œ:")
    print("  - 50+ í•œêµ­ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
    print("  - 85% ì´ìƒ ê´€ë ¨ì„± ì •í™•ë„")
    print("  - 2% ë¯¸ë§Œ ì¤‘ë³µë¥ ")
    print("  - Mock ë°ì´í„° ì™„ì „ ë°°ì œ")
    print("=" * 70)
    
    # ê²€ìƒ‰ì–´ ì…ë ¥ (ê¸°ë³¸ê°’: êµ­ë¯¼ì—°ê¸ˆ). ë¹„ëŒ€í™”í˜• ì‹¤í–‰ì„ ìœ„í•´ CLI ì¸ì ìš°ì„ .
    query = None
    # ê°„ë‹¨í•œ ì¸ì íŒŒì‹±
    args = sys.argv[1:]
    for i, a in enumerate(args):
        if a in ("--query", "-q") and i + 1 < len(args):
            query = args[i + 1]
            break
    if not query:
        # ëŒ€í™”í˜• ì…ë ¥ ì‹œë„ (í„°ë¯¸ë„ì´ ì¸í„°ë™í‹°ë¸Œí•œ ê²½ìš°)
        try:
            query = input("\nê²€ìƒ‰ì–´ ì…ë ¥ (Enter: êµ­ë¯¼ì—°ê¸ˆ): ").strip()
        except Exception:
            query = "êµ­ë¯¼ì—°ê¸ˆ"
    if not query:
        query = "êµ­ë¯¼ì—°ê¸ˆ"
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = IntegratedCrawlerSystem()
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        print(f"\nğŸ” '{query}' í¬ë¡¤ë§ ì‹œì‘...")
        results = await system.crawl_with_filtering(query)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 70)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
        print("=" * 70)
        
        stats = results['statistics']
        print(f"\nâœ… ì„±ëŠ¥ ì§€í‘œ:")
        print(f"  â€¢ ì†ŒìŠ¤ ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
        print(f"  â€¢ ê´€ë ¨ì„± ì •í™•ë„: {stats['relevance_rate']:.1f}%")
        print(f"  â€¢ ì¤‘ë³µ ì œê±°ìœ¨: {stats['duplicate_rate']:.1f}%")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {stats['duration_seconds']:.1f}ì´ˆ")
        
        print(f"\nğŸ“ˆ ìˆ˜ì§‘ í†µê³„:")
        print(f"  â€¢ í¬ë¡¤ë§ ì†ŒìŠ¤: {stats['successful_sources']}/{stats['total_sources']}")
        print(f"  â€¢ ì´ ìˆ˜ì§‘ í•­ëª©: {stats['total_articles_crawled']}ê°œ")
        print(f"  â€¢ ê´€ë ¨ í•­ëª©: {stats['relevant_articles']}ê°œ")
        print(f"  â€¢ ìµœì¢… ê³ ìœ  í•­ëª©: {stats['unique_articles']}ê°œ")
        
        print(f"\nğŸ“° ì†ŒìŠ¤ë³„ ì„±ê³¼ (ìƒìœ„ 5ê°œ):")
        for source in results['sources_summary'][:5]:
            print(
                f"  â€¢ {source['source_name']}: "
                f"{source['relevant_articles']}/{source['total_articles']} "
                f"({source['relevance_rate']:.1f}%)"
            )
        
        print(f"\nğŸ† ìƒìœ„ ê´€ë ¨ ê¸°ì‚¬ (Top 5):")
        for i, article in enumerate(results['articles'][:5], 1):
            print(f"\n  {i}. {article.get('title', 'Unknown')}")
            print(f"     ê´€ë ¨ë„: {article.get('relevance_score', 0):.2f}")
            print(f"     ì¶œì²˜: {article.get('platform', 'Unknown')}")
            if 'url' in article:
                print(f"     URL: {article['url'][:60]}...")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"integrated_result_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # PRD ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        print("\n" + "=" * 70)
        print("ğŸ¯ PRD ëª©í‘œ ë‹¬ì„±ë„")
        print("=" * 70)
        
        achievements = [
            ("ê´€ë ¨ì„± ì •í™•ë„ 85% ì´ìƒ", stats['relevance_rate'] >= 85, stats['relevance_rate']),
            ("ì¤‘ë³µë¥  2% ë¯¸ë§Œ", stats['duplicate_rate'] < 2, stats['duplicate_rate']),
            ("Mock ë°ì´í„° 0%", stats.get('crawler_stats', {}).get('mock_data_filtered', 0) == 0, 0),
            ("ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€ 10+", stats['successful_sources'] >= 10, stats['successful_sources'])
        ]
        
        for goal, achieved, value in achievements:
            status = "âœ…" if achieved else "âŒ"
            print(f"  {status} {goal}: {value:.1f}{'%' if '%' in goal else ''}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        system.cleanup()
        print("\nì‹œìŠ¤í…œ ì¢…ë£Œ")


if __name__ == "__main__":
    asyncio.run(main())
