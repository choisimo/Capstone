#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ì‹¤ì œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any
import feedparser
import hashlib

# ë°ì´í„° ìˆ˜ì§‘ ì†ŒìŠ¤ ì„¤ì •
DATA_SOURCES = {
    "rss_feeds": [
        {
            "name": "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ë‰´ìŠ¤",
            "url": "https://www.nps.or.kr/jsppage/cyber_pr/news/rss.jsp",
            "category": "official"
        },
        {
            "name": "ë³´ê±´ë³µì§€ë¶€ ë³´ë„ìë£Œ",
            "url": "https://www.mohw.go.kr/rss/news.xml",
            "category": "government"
        },
        {
            "name": "ì—°í•©ë‰´ìŠ¤ ê²½ì œ",
            "url": "https://www.yonhapnewstv.co.kr/category/news/economy/feed/",
            "category": "news"
        },
        {
            "name": "í•œê²¨ë ˆ ê²½ì œ",
            "url": "http://www.hani.co.kr/rss/economy/",
            "category": "news"
        },
        {
            "name": "ì¡°ì„ ì¼ë³´ ê²½ì œ",
            "url": "https://www.chosun.com/arc/outboundfeeds/rss/category/economy/?outputType=xml",
            "category": "news"
        }
    ],
    "reddit": {
        "subreddits": ["korea", "hanguk", "Korean", "Living_in_Korea"],
        "keywords": ["êµ­ë¯¼ì—°ê¸ˆ", "ì—°ê¸ˆ", "pension", "retirement", "ë…¸í›„"]
    },
    "news_comments": [
        {
            "name": "ë„¤ì´ë²„ ë‰´ìŠ¤",
            "search_url": "https://search.naver.com/search.naver?where=news&query=êµ­ë¯¼ì—°ê¸ˆ",
            "category": "portal"
        },
        {
            "name": "ë‹¤ìŒ ë‰´ìŠ¤",
            "search_url": "https://search.daum.net/search?w=news&q=êµ­ë¯¼ì—°ê¸ˆ",
            "category": "portal"
        }
    ]
}

def generate_user_id(author: str, platform: str) -> str:
    """ì‚¬ìš©ì ID ìƒì„±"""
    return hashlib.md5(f"{platform}:{author}".encode()).hexdigest()[:16]

def collect_rss_feeds() -> List[Dict[str, Any]]:
    """RSS í”¼ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    collected_data = []
    
    for feed_info in DATA_SOURCES["rss_feeds"]:
        print(f"\nğŸ“¡ ìˆ˜ì§‘ ì¤‘: {feed_info['name']}")
        print(f"   URL: {feed_info['url']}")
        
        try:
            feed = feedparser.parse(feed_info['url'])
            
            if not feed.entries:
                print(f"   âš ï¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤")
                continue
                
            for entry in feed.entries[:20]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 20ê°œ
                # êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ í•­ëª©ë§Œ í•„í„°ë§
                if any(keyword in (entry.get('title', '') + entry.get('summary', '')).lower() 
                       for keyword in ['ì—°ê¸ˆ', 'êµ­ë¯¼ì—°ê¸ˆ', 'ë…¸í›„', 'í‡´ì§', 'pension', 'ì€í‡´']):
                    
                    data = {
                        "id": hashlib.md5(entry.get('link', '').encode()).hexdigest()[:16],
                        "source": feed_info['name'],
                        "category": feed_info['category'],
                        "platform": "rss",
                        "title": entry.get('title', ''),
                        "content": entry.get('summary', '')[:500],
                        "url": entry.get('link', ''),
                        "author": entry.get('author', feed.feed.get('title', 'Unknown')),
                        "author_id": generate_user_id(
                            entry.get('author', feed.feed.get('title', 'Unknown')), 
                            "rss"
                        ),
                        "published_at": entry.get('published', datetime.now().isoformat()),
                        "collected_at": datetime.now().isoformat()
                    }
                    collected_data.append(data)
                    print(f"   âœ… ìˆ˜ì§‘: {data['title'][:50]}...")
                    
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return collected_data

def collect_reddit_data() -> List[Dict[str, Any]]:
    """Redditì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ê³µê°œ API)"""
    collected_data = []
    headers = {'User-Agent': 'PensionSentimentBot/1.0'}
    
    for subreddit in DATA_SOURCES["reddit"]["subreddits"]:
        print(f"\nğŸ¤– Reddit ìˆ˜ì§‘: r/{subreddit}")
        
        try:
            # Redditì˜ ê³µê°œ JSON API ì‚¬ìš©
            url = f"https://www.reddit.com/r/{subreddit}/search.json?q=pension+OR+ì—°ê¸ˆ&limit=25&sort=new"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"   âš ï¸ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                continue
                
            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post['data']
                
                collected_item = {
                    "id": post_data['id'],
                    "source": f"reddit_{subreddit}",
                    "category": "social",
                    "platform": "reddit",
                    "title": post_data.get('title', ''),
                    "content": post_data.get('selftext', '')[:1000],
                    "url": f"https://reddit.com{post_data.get('permalink', '')}",
                    "author": post_data.get('author', 'Unknown'),
                    "author_id": generate_user_id(post_data.get('author', 'Unknown'), "reddit"),
                    "score": post_data.get('score', 0),
                    "num_comments": post_data.get('num_comments', 0),
                    "published_at": datetime.fromtimestamp(post_data.get('created_utc', 0)).isoformat(),
                    "collected_at": datetime.now().isoformat()
                }
                collected_data.append(collected_item)
                print(f"   âœ… ìˆ˜ì§‘: {collected_item['title'][:50]}...")
                
            time.sleep(2)  # Rate limiting
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return collected_data

def generate_sample_comments() -> List[Dict[str, Any]]:
    """ìƒ˜í”Œ ëŒ“ê¸€ ìƒì„±ì€ ì •ì±…ìƒ ë¹„í™œì„±í™” (REAL DATA ONLY)."""
    return []

def analyze_collected_data(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„"""
    analysis = {
        "total_count": len(data),
        "by_platform": {},
        "by_category": {},
        "by_author": {},
        "time_range": {
            "earliest": None,
            "latest": None
        },
        "top_authors": []
    }
    
    # í”Œë«í¼ë³„ ì§‘ê³„
    for item in data:
        platform = item.get('platform', 'unknown')
        analysis['by_platform'][platform] = analysis['by_platform'].get(platform, 0) + 1
        
        category = item.get('category', 'unknown')
        analysis['by_category'][category] = analysis['by_category'].get(category, 0) + 1
        
        author = item.get('author', 'Unknown')
        if author not in analysis['by_author']:
            analysis['by_author'][author] = {
                'count': 0,
                'author_id': item.get('author_id'),
                'posts': []
            }
        analysis['by_author'][author]['count'] += 1
        analysis['by_author'][author]['posts'].append(item['id'])
    
    # Top authors
    sorted_authors = sorted(analysis['by_author'].items(), key=lambda x: x[1]['count'], reverse=True)
    analysis['top_authors'] = [
        {
            'name': author,
            'id': info['author_id'],
            'post_count': info['count']
        }
        for author, info in sorted_authors[:10]
    ]
    
    # ì‹œê°„ ë²”ìœ„
    dates = [item.get('published_at', '') for item in data if item.get('published_at')]
    if dates:
        analysis['time_range']['earliest'] = min(dates)
        analysis['time_range']['latest'] = max(dates)
    
    return analysis

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    
    all_data = []
    
    # 1. RSS í”¼ë“œ ìˆ˜ì§‘
    print("\n[1/3] RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    rss_data = collect_rss_feeds()
    all_data.extend(rss_data)
    print(f"âœ… RSS ìˆ˜ì§‘ ì™„ë£Œ: {len(rss_data)}ê°œ")
    
    # 2. Reddit ë°ì´í„° ìˆ˜ì§‘
    print("\n[2/3] Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    reddit_data = collect_reddit_data()
    all_data.extend(reddit_data)
    print(f"âœ… Reddit ìˆ˜ì§‘ ì™„ë£Œ: {len(reddit_data)}ê°œ")
    
    # 3. ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘ (ë¹„í™œì„±í™”) - ì‹¤ì œ ì†ŒìŠ¤ API ì—°ë™ í•„ìš” ì‹œ ë³„ë„ êµ¬í˜„
    print("\n[3/3] ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘ì€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (REAL DATA ONLY ì •ì±…)")
    
    # ë¶„ì„ ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ë¶„ì„")
    print("=" * 60)
    
    analysis = analyze_collected_data(all_data)
    
    print(f"\nì´ ìˆ˜ì§‘ ë°ì´í„°: {analysis['total_count']}ê°œ")
    
    print("\ní”Œë«í¼ë³„ ë¶„í¬:")
    for platform, count in analysis['by_platform'].items():
        print(f"  - {platform}: {count}ê°œ")
    
    print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in analysis['by_category'].items():
        print(f"  - {category}: {count}ê°œ")
    
    print("\nìƒìœ„ ì‘ì„±ì (Top 10):")
    for i, author in enumerate(analysis['top_authors'], 1):
        print(f"  {i}. {author['name']}: {author['post_count']}ê°œ (ID: {author['id']})")
    
    # ë°ì´í„° ì €ì¥
    output_file = f"/home/nodove/workspace/Capstone/data/collected_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    import os
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'collected_at': datetime.now().isoformat(),
                'total_count': len(all_data),
                'analysis': analysis
            },
            'data': all_data
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5ê°œ)")
    print("=" * 60)
    
    for i, item in enumerate(all_data[:5], 1):
        print(f"\n[{i}] {item.get('title', item.get('content', ''))[:100]}...")
        print(f"   ì‘ì„±ì: {item['author']} (ID: {item['author_id']})")
        print(f"   ì¶œì²˜: {item['source']} | í”Œë«í¼: {item['platform']}")
        print(f"   URL: {item.get('url', 'N/A')}")
    
    return all_data

if __name__ == "__main__":
    collected_data = main()
    print(f"\nâœ¨ ì™„ë£Œ! ì´ {len(collected_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
