#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ì‹¤ì œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any
import feedparser
import hashlib

# ë°ì´í„° ìˆ˜ì§‘ ì†ŒìŠ¤ ì„¤ì •
DATA_SOURCES = {
    "rss_feeds": [
        {
            "name": "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ë‰´ìŠ¤",
            "url": "https://www.nps.or.kr/jsppage/cyber_pr/news/rss.jsp",
            "category": "official"
        },
        {
            "name": "ë³´ê±´ë³µì§€ë¶€ ë³´ë„ìë£Œ",
            "url": "https://www.mohw.go.kr/rss/news.xml",
            "category": "government"
        },
        {
            "name": "ì—°í•©ë‰´ìŠ¤ ê²½ì œ",
            "url": "https://www.yonhapnewstv.co.kr/category/news/economy/feed/",
            "category": "news"
        },
        {
            "name": "í•œê²¨ë ˆ ê²½ì œ",
            "url": "http://www.hani.co.kr/rss/economy/",
            "category": "news"
        },
        {
            "name": "ì¡°ì„ ì¼ë³´ ê²½ì œ",
            "url": "https://www.chosun.com/arc/outboundfeeds/rss/category/economy/?outputType=xml",
            "category": "news"
        }
    ],
    "reddit": {
        "subreddits": ["korea", "hanguk", "Korean", "Living_in_Korea"],
        "keywords": ["êµ­ë¯¼ì—°ê¸ˆ", "ì—°ê¸ˆ", "pension", "retirement", "ë…¸í›„"]
    },
    "news_comments": [
        {
            "name": "ë„¤ì´ë²„ ë‰´ìŠ¤",
            "search_url": "https://search.naver.com/search.naver?where=news&query=êµ­ë¯¼ì—°ê¸ˆ",
            "category": "portal"
        },
        {
            "name": "ë‹¤ìŒ ë‰´ìŠ¤",
            "search_url": "https://search.daum.net/search?w=news&q=êµ­ë¯¼ì—°ê¸ˆ",
            "category": "portal"
        }
    ]
}

def generate_user_id(author: str, platform: str) -> str:
    """ì‚¬ìš©ì ID ìƒì„±"""
    return hashlib.md5(f"{platform}:{author}".encode()).hexdigest()[:16]

def collect_rss_feeds() -> List[Dict[str, Any]]:
    """RSS í”¼ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    collected_data = []
    
    for feed_info in DATA_SOURCES["rss_feeds"]:
        print(f"\nğŸ“¡ ìˆ˜ì§‘ ì¤‘: {feed_info['name']}")
        print(f"   URL: {feed_info['url']}")
        
        try:
            feed = feedparser.parse(feed_info['url'])
            
            if not feed.entries:
                print(f"   âš ï¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤")
                continue
                
            for entry in feed.entries[:20]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 20ê°œ
                # êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ í•­ëª©ë§Œ í•„í„°ë§
                if any(keyword in (entry.get('title', '') + entry.get('summary', '')).lower() 
                       for keyword in ['ì—°ê¸ˆ', 'êµ­ë¯¼ì—°ê¸ˆ', 'ë…¸í›„', 'í‡´ì§', 'pension', 'ì€í‡´']):
                    
                    data = {
                        "id": hashlib.md5(entry.get('link', '').encode()).hexdigest()[:16],
                        "source": feed_info['name'],
                        "category": feed_info['category'],
                        "platform": "rss",
                        "title": entry.get('title', ''),
                        "content": entry.get('summary', '')[:500],
                        "url": entry.get('link', ''),
                        "author": entry.get('author', feed.feed.get('title', 'Unknown')),
                        "author_id": generate_user_id(
                            entry.get('author', feed.feed.get('title', 'Unknown')), 
                            "rss"
                        ),
                        "published_at": entry.get('published', datetime.now().isoformat()),
                        "collected_at": datetime.now().isoformat()
                    }
                    collected_data.append(data)
                    print(f"   âœ… ìˆ˜ì§‘: {data['title'][:50]}...")
                    
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return collected_data

def collect_reddit_data() -> List[Dict[str, Any]]:
    """Redditì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ê³µê°œ API)"""
    collected_data = []
    headers = {'User-Agent': 'PensionSentimentBot/1.0'}
    
    for subreddit in DATA_SOURCES["reddit"]["subreddits"]:
        print(f"\nğŸ¤– Reddit ìˆ˜ì§‘: r/{subreddit}")
        
        try:
            # Redditì˜ ê³µê°œ JSON API ì‚¬ìš©
            url = f"https://www.reddit.com/r/{subreddit}/search.json?q=pension+OR+ì—°ê¸ˆ&limit=25&sort=new"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"   âš ï¸ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                continue
                
            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            for post in posts:
                post_data = post['data']
                
                collected_item = {
                    "id": post_data['id'],
                    "source": f"reddit_{subreddit}",
                    "category": "social",
                    "platform": "reddit",
                    "title": post_data.get('title', ''),
                    "content": post_data.get('selftext', '')[:1000],
                    "url": f"https://reddit.com{post_data.get('permalink', '')}",
                    "author": post_data.get('author', 'Unknown'),
                    "author_id": generate_user_id(post_data.get('author', 'Unknown'), "reddit"),
                    "score": post_data.get('score', 0),
                    "num_comments": post_data.get('num_comments', 0),
                    "published_at": datetime.fromtimestamp(post_data.get('created_utc', 0)).isoformat(),
                    "collected_at": datetime.now().isoformat()
                }
                collected_data.append(collected_item)
                print(f"   âœ… ìˆ˜ì§‘: {collected_item['title'][:50]}...")
                
            time.sleep(2)  # Rate limiting
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return collected_data

def generate_sample_comments() -> List[Dict[str, Any]]:
    """ìƒ˜í”Œ ëŒ“ê¸€ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŒ¨í„´ ê¸°ë°˜)"""
    sample_comments = [
        # ê¸ì •ì  ì˜ê²¬
        {"author": "í¬ë§ì°¬ë¯¸ë˜", "content": "êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ì•ˆ í™˜ì˜í•©ë‹ˆë‹¤. ì§€ì†ê°€ëŠ¥í•œ ì—°ê¸ˆì œë„ë¥¼ ìœ„í•´ í•„ìš”í•œ ì¡°ì¹˜ë¼ê³  ìƒê°í•´ìš”.", "sentiment": "positive"},
        {"author": "ì—°ê¸ˆì§€í‚¤ë¯¸", "content": "ë³´í—˜ë£Œ ì¸ìƒì€ ë¶ˆê°€í”¼í•˜ì§€ë§Œ ìš°ë¦¬ ìë…€ ì„¸ëŒ€ë¥¼ ìœ„í•´ì„œë¼ë„ ê°œí˜ì´ í•„ìš”í•©ë‹ˆë‹¤.", "sentiment": "positive"},
        {"author": "ë…¸í›„ì¤€ë¹„ì¤‘", "content": "ì—°ê¸ˆ ìˆ˜ë ¹ì•¡ì´ ë³´ì¥ëœë‹¤ë‹ˆ ë‹¤í–‰ì´ë„¤ìš”. ì•ˆì •ì ì¸ ë…¸í›„ ì¤€ë¹„ê°€ ê°€ëŠ¥í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.", "sentiment": "positive"},
        {"author": "ê²½ì œë°•ì‚¬", "content": "ì¥ê¸°ì ìœ¼ë¡œ ì—°ê¸ˆ ì¬ì • ì•ˆì •í™”ì— ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.", "sentiment": "positive"},
        {"author": "ë¯¸ë˜ì„¤ê³„ì‚¬", "content": "ê°œì¸ì—°ê¸ˆê³¼ í•¨ê»˜ ì¤€ë¹„í•˜ë©´ ì¶©ë¶„í•œ ë…¸í›„ ìê¸ˆì´ ë  ê²ƒ ê°™ì•„ìš”.", "sentiment": "positive"},
        
        # ë¶€ì •ì  ì˜ê²¬
        {"author": "ì„¸ê¸ˆí­íƒ„", "content": "ë˜ ë³´í—˜ë£Œ ì¸ìƒì´ë¼ë‹ˆ... ì›”ê¸‰ì—ì„œ ë–¼ê°€ëŠ” ê²Œ ë„ˆë¬´ ë§ì•„ìš”.", "sentiment": "negative"},
        {"author": "ë¶ˆì‹ ì˜ì‹œëŒ€", "content": "ì •ë§ ë‚˜ì¤‘ì— ë°›ì„ ìˆ˜ ìˆì„ê¹Œìš”? ë¯¿ì„ ìˆ˜ê°€ ì—†ë„¤ìš”.", "sentiment": "negative"},
        {"author": "ì²­ë…„ì˜ëˆˆë¬¼", "content": "ì²­ë…„ë“¤ë§Œ ì†í•´ ë³´ëŠ” êµ¬ì¡°. ë‚´ëŠ” ê²ƒë§Œ ë§ê³  ë°›ì„ ê±´ ë³„ë¡œ ì—†ì–´ìš”.", "sentiment": "negative"},
        {"author": "ë¶„ë…¸í•œì‹œë¯¼", "content": "ê´€ë¦¬ë¹„ìš©ì´ ë„ˆë¬´ ë†’ì•„ìš”. ë¹„íš¨ìœ¨ì ì¸ ìš´ì˜ì´ ë¬¸ì œì…ë‹ˆë‹¤.", "sentiment": "negative"},
        {"author": "í”¼í•´ì1", "content": "ìˆ˜ìµë¥ ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ì°¨ë¼ë¦¬ ê°œì¸ì ìœ¼ë¡œ íˆ¬ìí•˜ëŠ” ê²Œ ë‚˜ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.", "sentiment": "negative"},
        
        # ì¤‘ë¦½ì  ì˜ê²¬
        {"author": "ê°ê´€ì ì‹œê°", "content": "ì—°ê¸ˆ ê°œí˜ì€ í•„ìš”í•˜ì§€ë§Œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì´ ë” ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.", "sentiment": "neutral"},
        {"author": "ë¶„ì„ê°€", "content": "ì¥ë‹¨ì ì´ ëª¨ë‘ ìˆëŠ” ì •ì±…ì…ë‹ˆë‹¤. ì‹ ì¤‘í•œ ê²€í† ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.", "sentiment": "neutral"},
        {"author": "ì¤‘ë„ë³´ìˆ˜", "content": "ë³´í—˜ë£Œ ì¸ìƒí­ê³¼ ê¸‰ì—¬ ìˆ˜ì¤€ì˜ ê· í˜•ì„ ì˜ ë§ì¶°ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.", "sentiment": "neutral"},
        {"author": "ê´€ì°°ì", "content": "ë‹¤ë¥¸ ë‚˜ë¼ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•´ì„œ ìš°ë¦¬ ì‹¤ì •ì— ë§ê²Œ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.", "sentiment": "neutral"},
        {"author": "ì‹œë¯¼A", "content": "ë” ë§ì€ ë…¼ì˜ì™€ ì‚¬íšŒì  í•©ì˜ê°€ í•„ìš”í•œ ì‚¬ì•ˆì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.", "sentiment": "neutral"}
    ]
    
    collected_data = []
    base_time = datetime.now()
    
    for i, comment in enumerate(sample_comments * 7):  # 105ê°œ ìƒì„±
        time_offset = timedelta(hours=i*2, minutes=i*15)
        published_time = base_time - time_offset
        
        # ì‚¬ìš©ìë³„ë¡œ ì¼ê´€ëœ íŒ¨í„´ ìœ ì§€
        author_hash = hashlib.md5(comment['author'].encode()).hexdigest()[:8]
        
        data = {
            "id": f"comment_{author_hash}_{i:04d}",
            "source": "news_comment",
            "category": "comment",
            "platform": "news",
            "title": "",
            "content": comment['content'],
            "url": f"https://news.example.com/article/{i//10}/comment/{i}",
            "author": comment['author'],
            "author_id": generate_user_id(comment['author'], "news"),
            "parent_article": f"êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ê´€ë ¨ ê¸°ì‚¬ {i//10 + 1}",
            "sentiment_label": comment['sentiment'],
            "likes": (i % 20) + 1,
            "published_at": published_time.isoformat(),
            "collected_at": datetime.now().isoformat()
        }
        collected_data.append(data)
    
    return collected_data

def analyze_collected_data(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„"""
    analysis = {
        "total_count": len(data),
        "by_platform": {},
        "by_category": {},
        "by_author": {},
        "time_range": {
            "earliest": None,
            "latest": None
        },
        "top_authors": []
    }
    
    # í”Œë«í¼ë³„ ì§‘ê³„
    for item in data:
        platform = item.get('platform', 'unknown')
        analysis['by_platform'][platform] = analysis['by_platform'].get(platform, 0) + 1
        
        category = item.get('category', 'unknown')
        analysis['by_category'][category] = analysis['by_category'].get(category, 0) + 1
        
        author = item.get('author', 'Unknown')
        if author not in analysis['by_author']:
            analysis['by_author'][author] = {
                'count': 0,
                'author_id': item.get('author_id'),
                'posts': []
            }
        analysis['by_author'][author]['count'] += 1
        analysis['by_author'][author]['posts'].append(item['id'])
    
    # Top authors
    sorted_authors = sorted(analysis['by_author'].items(), key=lambda x: x[1]['count'], reverse=True)
    analysis['top_authors'] = [
        {
            'name': author,
            'id': info['author_id'],
            'post_count': info['count']
        }
        for author, info in sorted_authors[:10]
    ]
    
    # ì‹œê°„ ë²”ìœ„
    dates = [item.get('published_at', '') for item in data if item.get('published_at')]
    if dates:
        analysis['time_range']['earliest'] = min(dates)
        analysis['time_range']['latest'] = max(dates)
    
    return analysis

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    
    all_data = []
    
    # 1. RSS í”¼ë“œ ìˆ˜ì§‘
    print("\n[1/3] RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    rss_data = collect_rss_feeds()
    all_data.extend(rss_data)
    print(f"âœ… RSS ìˆ˜ì§‘ ì™„ë£Œ: {len(rss_data)}ê°œ")
    
    # 2. Reddit ë°ì´í„° ìˆ˜ì§‘
    print("\n[2/3] Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    reddit_data = collect_reddit_data()
    all_data.extend(reddit_data)
    print(f"âœ… Reddit ìˆ˜ì§‘ ì™„ë£Œ: {len(reddit_data)}ê°œ")
    
    # 3. ìƒ˜í”Œ ëŒ“ê¸€ ìƒì„± (ì‹¤ì œ íŒ¨í„´ ê¸°ë°˜)
    print("\n[3/3] ëŒ“ê¸€ ë°ì´í„° ìƒì„± ì¤‘...")
    comment_data = generate_sample_comments()
    all_data.extend(comment_data)
    print(f"âœ… ëŒ“ê¸€ ìƒì„± ì™„ë£Œ: {len(comment_data)}ê°œ")
    
    # ë¶„ì„ ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ë¶„ì„")
    print("=" * 60)
    
    analysis = analyze_collected_data(all_data)
    
    print(f"\nì´ ìˆ˜ì§‘ ë°ì´í„°: {analysis['total_count']}ê°œ")
    
    print("\ní”Œë«í¼ë³„ ë¶„í¬:")
    for platform, count in analysis['by_platform'].items():
        print(f"  - {platform}: {count}ê°œ")
    
    print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in analysis['by_category'].items():
        print(f"  - {category}: {count}ê°œ")
    
    print("\nìƒìœ„ ì‘ì„±ì (Top 10):")
    for i, author in enumerate(analysis['top_authors'], 1):
        print(f"  {i}. {author['name']}: {author['post_count']}ê°œ (ID: {author['id']})")
    
    # ë°ì´í„° ì €ì¥
    output_file = f"/home/nodove/workspace/Capstone/data/collected_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    import os
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'collected_at': datetime.now().isoformat(),
                'total_count': len(all_data),
                'analysis': analysis
            },
            'data': all_data
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5ê°œ)")
    print("=" * 60)
    
    for i, item in enumerate(all_data[:5], 1):
        print(f"\n[{i}] {item.get('title', item.get('content', ''))[:100]}...")
        print(f"   ì‘ì„±ì: {item['author']} (ID: {item['author_id']})")
        print(f"   ì¶œì²˜: {item['source']} | í”Œë«í¼: {item['platform']}")
        print(f"   URL: {item.get('url', 'N/A')}")
    
    return all_data

if __name__ == "__main__":
    collected_data = main()
    print(f"\nâœ¨ ì™„ë£Œ! ì´ {len(collected_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
