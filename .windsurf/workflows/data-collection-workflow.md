---
description: ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°
---

# ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤

### Step 1: ë°ì´í„° ì†ŒìŠ¤ í™•ì¸
```bash
// turbo
# 1. ê³µì‹ ì‚¬ì´íŠ¸ í™•ì¸
curl -I https://www.nps.or.kr  # êµ­ë¯¼ì—°ê¸ˆê³µë‹¨
curl -I https://www.mohw.go.kr  # ë³´ê±´ë³µì§€ë¶€
```

### Step 2: URL ê²€ì¦
```python
# ìˆ˜ì§‘ ì „ URLì´ ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
import requests

def verify_url_exists(url):
    try:
        response = requests.head(url, timeout=5)
        return response.status_code < 400
    except:
        return False

# ê²€ì¦ í›„ ìˆ˜ì§‘
if verify_url_exists(url):
    # ìˆ˜ì§‘ ì§„í–‰
    pass
```

### Step 3: ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
```python
# RSS í”¼ë“œ ìˆ˜ì§‘ ì˜ˆì‹œ
import feedparser

# ì‹¤ì œ RSS URLë§Œ ì‚¬ìš©
rss_urls = [
    "https://www.nps.or.kr/jsppage/cyber_pr/news/rss.jsp",  # êµ­ë¯¼ì—°ê¸ˆê³µë‹¨
    "https://www.mohw.go.kr/rss/news.xml"  # ë³´ê±´ë³µì§€ë¶€
]

for url in rss_urls:
    if verify_url_exists(url):
        feed = feedparser.parse(url)
        # ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬
```

### Step 4: ë°ì´í„° ê²€ì¦
```python
def validate_collected_data(data):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦"""
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    required = ['url', 'source', 'title', 'collected_at']
    
    for field in required:
        if field not in data or not data[field]:
            return False
    
    # URL í˜•ì‹ í™•ì¸
    if not data['url'].startswith(('http://', 'https://')):
        return False
    
    # ì‹¤ì œ URLì¸ì§€ ì¬í™•ì¸
    if not verify_url_exists(data['url']):
        return False
    
    return True
```

### Step 5: ë°ì´í„° ì €ì¥
```python
# ê²€ì¦ëœ ë°ì´í„°ë§Œ ì €ì¥
validated_data = []

for item in collected_data:
    if validate_collected_data(item):
        validated_data.append(item)
    else:
        print(f"Invalid data rejected: {item.get('url', 'unknown')}")

# ì €ì¥ ì‹œ ë©”íƒ€ë°ì´í„° í¬í•¨
save_data = {
    "metadata": {
        "source": "real_websites",
        "collected_at": datetime.now().isoformat(),
        "total_items": len(validated_data),
        "all_urls_verified": True
    },
    "data": validated_data
}
```

## ğŸš« í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ

### 1. ê°€ì§œ URL ìƒì„±
```python
# âŒ ì ˆëŒ€ ê¸ˆì§€
fake_url = f"https://example.com/post/{random.randint(1000, 9999)}"
```

### 2. Mock ë°ì´í„° ìƒì„±
```python
# âŒ ì ˆëŒ€ ê¸ˆì§€
mock_comment = {
    "author": f"user_{random.randint(1, 100)}",
    "content": "í…ŒìŠ¤íŠ¸ ëŒ“ê¸€ì…ë‹ˆë‹¤",
    "url": "https://fake-site.com/comment/123"
}
```

### 3. ëœë¤ ê°’ ìƒì„±
```python
# âŒ ì ˆëŒ€ ê¸ˆì§€
likes = random.randint(0, 1000)
views = random.randint(100, 10000)
```

## âœ… ì˜¬ë°”ë¥¸ ë°ì´í„° ì†ŒìŠ¤

### 1. ê³µì‹ API
- êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ ê³µê°œ API
- ê³µê³µë°ì´í„°í¬í„¸ API
- ì •ë¶€ ë¶€ì²˜ RSS í”¼ë“œ

### 2. ì‹¤ì œ ì›¹í˜ì´ì§€
- robots.txt ì¤€ìˆ˜
- ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€
- User-Agent ëª…ì‹œ

### 3. ê³µê°œ ë°ì´í„°ì…‹
- ì •ë¶€ ê³µê³µë°ì´í„°
- ì—°êµ¬ê¸°ê´€ ë°ì´í„°
- ì˜¤í”ˆ ë¼ì´ì„¼ìŠ¤ ë°ì´í„°

## ğŸ“ ë°ì´í„° ìˆ˜ì§‘ í…œí”Œë¦¿

```python
class RealDataCollector:
    """ì‹¤ì œ ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.verified_sources = [
            "https://www.nps.or.kr",
            "https://www.mohw.go.kr",
            # ê²€ì¦ëœ ì†ŒìŠ¤ë§Œ ì¶”ê°€
        ]
    
    def collect(self, source_url):
        # 1. URL ê²€ì¦
        if not self.verify_url(source_url):
            raise ValueError(f"Invalid URL: {source_url}")
        
        # 2. ë°ì´í„° ìˆ˜ì§‘
        data = self.fetch_data(source_url)
        
        # 3. ë°ì´í„° ê²€ì¦
        if not self.validate_data(data):
            raise ValueError(f"Invalid data from: {source_url}")
        
        # 4. ì›ë³¸ URL í¬í•¨í•˜ì—¬ ì €ì¥
        data['original_url'] = source_url
        data['verified'] = True
        data['collected_at'] = datetime.now().isoformat()
        
        return data
    
    def verify_url(self, url):
        """URLì´ ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        # ì‹¤ì œ HTTP ìš”ì²­ìœ¼ë¡œ í™•ì¸
        pass
    
    def fetch_data(self, url):
        """ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìŠ¤í¬ë˜í•‘
        pass
    
    def validate_data(self, data):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ, í˜•ì‹ ë“± í™•ì¸
        pass
```

## ğŸ” ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ë°ì´í„° ìˆ˜ì§‘ ì „:
- [ ] URLì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ê°€?
- [ ] robots.txtë¥¼ í™•ì¸í–ˆëŠ”ê°€?
- [ ] API ì‚¬ìš© ì•½ê´€ì„ í™•ì¸í–ˆëŠ”ê°€?

ë°ì´í„° ìˆ˜ì§‘ ì¤‘:
- [ ] ì‹¤ì œ ì‘ë‹µì„ ë°›ì•˜ëŠ”ê°€?
- [ ] ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?
- [ ] ì›ë³¸ URLì„ ê¸°ë¡í–ˆëŠ”ê°€?

ë°ì´í„° ìˆ˜ì§‘ í›„:
- [ ] ëª¨ë“  URLì´ ê²€ì¦ë˜ì—ˆëŠ”ê°€?
- [ ] ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í–ˆëŠ”ê°€?
- [ ] ì €ì¥ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?

## ğŸ’¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 403 Forbidden ì˜¤ë¥˜
```python
# User-Agent ì¶”ê°€
headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; DataCollector/1.0)'
}
response = requests.get(url, headers=headers)
```

### ì†ë„ ì œí•œ
```python
# ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
import time

for url in urls:
    data = collect(url)
    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
```

### SSL ì¸ì¦ì„œ ì˜¤ë¥˜
```python
# ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸ë§Œ
response = requests.get(url, verify=True)
```

---
**ì‹¤ì œ ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ì—¬ í”„ë¡œì íŠ¸ì˜ ì‹ ë¢°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.**
