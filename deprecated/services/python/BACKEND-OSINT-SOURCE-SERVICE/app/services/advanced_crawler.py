"""
ê³ ê¸‰ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ - ë„¤ì´ë²„, ë‹¤ìŒ, ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ ì™„ë²½ ì§€ì›
ë´‡ ì°¨ë‹¨ ìš°íšŒ ë° ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
"""

import asyncio
import aiohttp
from aiohttp import CookieJar
import requests
from bs4 import BeautifulSoup
import feedparser
import json
import hashlib
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from urllib.parse import quote, urljoin, urlparse
import re

class AdvancedCrawler:
    def __init__(self):
        # ë‹¤ì–‘í•œ User-Agent ë¡œí…Œì´ì…˜
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        # ê²°ì •ë¡ ì  ë¡œí…Œì´ì…˜ ì¸ë±ìŠ¤
        self._ua_index = 0
        
        # ë„¤ì´ë²„ ì „ìš© í—¤ë”
        self.naver_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.naver.com'
        }
        
        # ë‹¤ìŒ ì „ìš© í—¤ë”
        self.daum_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Connection': 'keep-alive',
            'Referer': 'https://www.daum.net',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        }
        
    def next_user_agent(self) -> str:
        """ê²°ì •ë¡ ì  User-Agent ë¡œí…Œì´ì…˜ (random ë¯¸ì‚¬ìš©)"""
        ua = self.user_agents[self._ua_index % len(self.user_agents)]
        self._ua_index += 1
        return ua
    
    async def fetch_with_retry(self, url: str, headers: Dict = None, max_retries: int = 3) -> Optional[str]:
        """ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ë¹„ë™ê¸° í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ë´‡ ê°ì§€ íšŒí”¼) - ê²°ì •ë¡ ì  ì§€ì—°
                if attempt > 0:
                    await asyncio.sleep(self._deterministic_delay(attempt, url))
                
                cookie_jar = CookieJar()
                timeout = aiohttp.ClientTimeout(total=30)
                
                async with aiohttp.ClientSession(cookie_jar=cookie_jar, timeout=timeout) as session:
                    # ì„¸ì…˜ ì¿ í‚¤ ì„¤ì •ì„ ìœ„í•œ ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
                    if 'naver.com' in url:
                        await session.get('https://www.naver.com', headers={'User-Agent': self.next_user_agent()})
                    elif 'daum.net' in url:
                        await session.get('https://www.daum.net', headers={'User-Agent': self.next_user_agent()})
                    
                    # ì‹¤ì œ ìš”ì²­
                    async with session.get(url, headers=headers) as response:
                        if response.status == 200:
                            return await response.text()
                        elif response.status == 403 or response.status == 429:
                            print(f"âš ï¸ Rate limited or blocked on attempt {attempt + 1}")
                            continue
            except Exception as e:
                print(f"âŒ Error on attempt {attempt + 1}: {e}")
                continue
        
        return None

    def _deterministic_delay(self, attempt: int, url: str) -> float:
        """URLê³¼ ì‹œë„ íšŸìˆ˜ ê¸°ë°˜ ê²°ì •ë¡ ì  ì§€ì—°(1.0~3.0ì´ˆ)"""
        seed = f"{url}:{attempt}"
        h = hashlib.sha256(seed.encode()).digest()
        val = int.from_bytes(h[:2], 'big') / 65535.0  # 0..1
        return 1.0 + 2.0 * val
    
    async def crawl_naver_news(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ê°œì„ ëœ ë°©ë²•"""
        print(f"\nğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§: {query}")
        
        # URL ì¸ì½”ë”©
        encoded_query = quote(query)
        
        # ì—¬ëŸ¬ ì ‘ê·¼ ë°©ë²• ì‹œë„
        strategies = [
            {
                'name': 'ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ìŠ¤íƒ€ì¼',
                'url': f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={encoded_query}',
                'selector': 'a.news_tit'
            },
            {
                'name': 'ë„¤ì´ë²„ ë‰´ìŠ¤ íƒ­',
                'url': f'https://search.naver.com/search.naver?where=news&query={encoded_query}&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=&is_sug_officeid=0',
                'selector': 'a.news_tit'
            },
            {
                'name': 'ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰',
                'url': f'https://search.naver.com/search.naver?query={encoded_query}',
                'selector': '.news_wrap a.news_tit'
            }
        ]
        
        results = []
        
        for strategy in strategies:
            headers = self.naver_headers.copy()
            headers['User-Agent'] = self.next_user_agent()
            
            html = await self.fetch_with_retry(strategy['url'], headers)
            
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                news_items = soup.select(strategy['selector'])
                
                if news_items:
                    print(f"  âœ… {strategy['name']}: {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                    
                    for item in news_items[:10]:
                        title = item.get_text(strip=True)
                        link = item.get('href', '')
                        
                        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                        parent = item.find_parent(['div', 'li'])
                        source = ''
                        date_text = ''
                        
                        if parent:
                            source_elem = parent.select_one('.info_group .press')
                            if source_elem:
                                source = source_elem.get_text(strip=True)
                            
                            date_elem = parent.select_one('.info_group .info')
                            if date_elem:
                                date_text = date_elem.get_text(strip=True)
                        
                        results.append({
                            'title': title,
                            'url': link,
                            'source': source,
                            'date': date_text,
                            'platform': 'ë„¤ì´ë²„'
                        })
                    
                    break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
                else:
                    print(f"  âš ï¸ {strategy['name']}: ê²°ê³¼ ì—†ìŒ")
        
        return {
            'platform': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
            'query': query,
            'count': len(results),
            'articles': results,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_daum_news(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ - ê°œì„ ëœ ë°©ë²•"""
        print(f"\nğŸ” ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§: {query}")
        
        encoded_query = quote(query)
        
        strategies = [
            {
                'name': 'ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰',
                'url': f'https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={encoded_query}',
                'selector': 'a.f_link_b'
            },
            {
                'name': 'ë‹¤ìŒ ë‰´ìŠ¤ ìµœì‹ ìˆœ',
                'url': f'https://search.daum.net/search?w=news&DA=STC&enc=utf8&cluster=y&cluster_page=1&q={encoded_query}',
                'selector': 'a.tit_main'
            }
        ]
        
        results = []
        
        for strategy in strategies:
            headers = self.daum_headers.copy()
            headers['User-Agent'] = self.next_user_agent()
            
            html = await self.fetch_with_retry(strategy['url'], headers)
            
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                news_items = soup.select(strategy['selector'])
                
                if news_items:
                    print(f"  âœ… {strategy['name']}: {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                    
                    for item in news_items[:10]:
                        title = item.get_text(strip=True)
                        link = item.get('href', '')
                        
                        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                        parent = item.find_parent(['div', 'li'])
                        source = ''
                        date_text = ''
                        
                        if parent:
                            source_elem = parent.select_one('.f_nb')
                            if source_elem:
                                source = source_elem.get_text(strip=True)
                            
                            date_elem = parent.select_one('.f_nb.date')
                            if date_elem:
                                date_text = date_elem.get_text(strip=True)
                        
                        results.append({
                            'title': title,
                            'url': link,
                            'source': source,
                            'date': date_text,
                            'platform': 'ë‹¤ìŒ'
                        })
                    
                    break
                else:
                    print(f"  âš ï¸ {strategy['name']}: ê²°ê³¼ ì—†ìŒ")
        
        return {
            'platform': 'ë‹¤ìŒ ë‰´ìŠ¤',
            'query': query,
            'count': len(results),
            'articles': results,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_community_sites(self) -> List[Dict]:
        """ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        print("\nğŸŒ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
        
        communities = [
            {
                'name': 'í´ë¦¬ì•™',
                'url': 'https://www.clien.net/service/board/park',
                'selector': 'span.subject_fixed',
                'link_selector': 'a.list_subject',
                'platform': 'clien'
            },
            {
                'name': 'ë½ë¿Œ',
                'url': 'https://www.ppomppu.co.kr/zboard/zboard.php?id=freeboard',
                'selector': 'font.list_title',
                'link_selector': 'a[href*="view.php"]',
                'platform': 'ppomppu'
            },
            {
                'name': 'MLBPark',
                'url': 'http://mlbpark.donga.com/mp/b.php?b=bullpen',
                'selector': 'a.bullpenbox',
                'link_selector': 'a.bullpenbox',
                'platform': 'mlbpark'
            },
            {
                'name': 'FMì½”ë¦¬ì•„',
                'url': 'https://www.fmkorea.com/index.php?mid=best',
                'selector': 'h3.title a',
                'link_selector': 'h3.title a',
                'platform': 'fmkorea'
            },
            {
                'name': 'ë£¨ë¦¬ì›¹',
                'url': 'https://bbs.ruliweb.com/best/board/300143',
                'selector': 'a.subject',
                'link_selector': 'a.subject',
                'platform': 'ruliweb'
            }
        ]
        
        all_results = []
        
        for site in communities:
            print(f"  ğŸ“ {site['name']} í¬ë¡¤ë§ ì¤‘...")
            
            headers = {
                'User-Agent': self.next_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9'
            }
            
            # ê° ì‚¬ì´íŠ¸ë³„ íŠ¹ë³„ ì²˜ë¦¬
            if site['platform'] == 'clien':
                headers['Referer'] = 'https://www.clien.net'
            elif site['platform'] == 'ppomppu':
                headers['Referer'] = 'https://www.ppomppu.co.kr'
            
            html = await self.fetch_with_retry(site['url'], headers)
            
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                posts = soup.select(site['selector'])[:20]
                
                if posts:
                    print(f"    âœ… {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                    
                    site_results = []
                    for post in posts[:10]:
                        title = post.get_text(strip=True)
                        
                        # ë§í¬ ì¶”ì¶œ
                        if site['link_selector'] != site['selector']:
                            link_elem = post.find_parent().select_one(site['link_selector'])
                        else:
                            link_elem = post
                        
                        link = ''
                        if link_elem and link_elem.get('href'):
                            link = urljoin(site['url'], link_elem.get('href'))
                        
                        site_results.append({
                            'title': title,
                            'url': link,
                            'platform': site['name'],
                            'source_url': site['url']
                        })
                    
                    all_results.append({
                        'site': site['name'],
                        'count': len(site_results),
                        'posts': site_results,
                        'timestamp': datetime.now().isoformat()
                    })
                else:
                    print(f"    âš ï¸ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"    âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
        
        return all_results
    
    async def crawl_naver_api(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ë„¤ì´ë²„ ê²€ìƒ‰ API ìŠ¤íƒ€ì¼ ì ‘ê·¼ (ê³µì‹ API ì•„ë‹˜)"""
        print(f"\nğŸ”§ ë„¤ì´ë²„ API ìŠ¤íƒ€ì¼ í¬ë¡¤ë§: {query}")
        
        # ëª¨ë°”ì¼ ë²„ì „ ì‹œë„ (ë” ê°„ë‹¨í•œ êµ¬ì¡°)
        mobile_url = f"https://m.search.naver.com/search.naver?where=m_news&query={quote(query)}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        }
        
        html = await self.fetch_with_retry(mobile_url, headers)
        
        results = []
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ëª¨ë°”ì¼ ë‰´ìŠ¤ ì„ íƒì
            news_items = soup.select('.news_wrap .bx')
            
            if news_items:
                print(f"  âœ… ëª¨ë°”ì¼ ë²„ì „ì—ì„œ {len(news_items)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                
                for item in news_items[:10]:
                    title_elem = item.select_one('.news_tit')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        
                        source_elem = item.select_one('.info_group .press')
                        source = source_elem.get_text(strip=True) if source_elem else ''
                        
                        date_elem = item.select_one('.info_group .info')
                        date_text = date_elem.get_text(strip=True) if date_elem else ''
                        
                        results.append({
                            'title': title,
                            'url': link,
                            'source': source,
                            'date': date_text,
                            'platform': 'ë„¤ì´ë²„ ëª¨ë°”ì¼'
                        })
        
        return {
            'platform': 'ë„¤ì´ë²„ API ìŠ¤íƒ€ì¼',
            'query': query,
            'count': len(results),
            'articles': results,
            'timestamp': datetime.now().isoformat()
        }
    
    async def crawl_all_sources(self, query: str = "êµ­ë¯¼ì—°ê¸ˆ") -> Dict:
        """ëª¨ë“  ì†ŒìŠ¤ í†µí•© í¬ë¡¤ë§"""
        print("=" * 60)
        print("ğŸš€ ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        tasks = [
            self.crawl_naver_news(query),
            self.crawl_naver_api(query),
            self.crawl_daum_news(query),
            self.crawl_community_sites()
        ]
        
        results = await asyncio.gather(*tasks)
        
        # êµ¬ê¸€ ë‰´ìŠ¤ RSS (ì´ë¯¸ ì‘ë™ í™•ì¸ë¨)
        google_rss_url = f"https://news.google.com/rss/search?q={quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(google_rss_url)
        
        google_results = {
            'platform': 'êµ¬ê¸€ ë‰´ìŠ¤ RSS',
            'query': query,
            'count': len(feed.entries),
            'articles': [
                {
                    'title': entry.title,
                    'url': entry.link,
                    'date': entry.get('published', ''),
                    'source': entry.source.title if hasattr(entry, 'source') else '',
                    'platform': 'êµ¬ê¸€ RSS'
                }
                for entry in feed.entries[:10]
            ],
            'timestamp': datetime.now().isoformat()
        }
        
        # í†µí•© ê²°ê³¼
        total_articles = sum(
            r.get('count', 0) if not isinstance(r, list) else sum(s.get('count', 0) for s in r)
            for r in results
        ) + google_results['count']
        
        return {
            'status': 'success',
            'total_articles': total_articles,
            'sources': {
                'naver': results[0],
                'naver_mobile': results[1],
                'daum': results[2],
                'communities': results[3],
                'google_rss': google_results
            },
            'timestamp': datetime.now().isoformat()
        }

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
async def test_advanced_crawler():
    crawler = AdvancedCrawler()
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸
    print("\n" + "="*60)
    print("ê°œë³„ ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # ë„¤ì´ë²„ í…ŒìŠ¤íŠ¸
    naver_results = await crawler.crawl_naver_news("êµ­ë¯¼ì—°ê¸ˆ")
    print(f"\nâœ… ë„¤ì´ë²„: {naver_results['count']}ê°œ ê¸°ì‚¬")
    for article in naver_results['articles'][:3]:
        print(f"  - {article['title'][:50]}...")
    
    # ë‹¤ìŒ í…ŒìŠ¤íŠ¸
    daum_results = await crawler.crawl_daum_news("êµ­ë¯¼ì—°ê¸ˆ")
    print(f"\nâœ… ë‹¤ìŒ: {daum_results['count']}ê°œ ê¸°ì‚¬")
    for article in daum_results['articles'][:3]:
        print(f"  - {article['title'][:50]}...")
    
    # ì»¤ë®¤ë‹ˆí‹° í…ŒìŠ¤íŠ¸
    community_results = await crawler.crawl_community_sites()
    print(f"\nâœ… ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸: {len(community_results)}ê°œ ì‚¬ì´íŠ¸")
    for site in community_results:
        if site['count'] > 0:
            print(f"  - {site['site']}: {site['count']}ê°œ ê²Œì‹œê¸€")
    
    # ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸
    print("\n" + "="*60)
    print("í†µí•© í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    all_results = await crawler.crawl_all_sources("êµ­ë¯¼ì—°ê¸ˆ")
    
    print(f"\nğŸ“Š ì´ ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"  - ì „ì²´ ê¸°ì‚¬/ê²Œì‹œê¸€: {all_results['total_articles']}ê°œ")
    print(f"  - ë„¤ì´ë²„: {all_results['sources']['naver']['count']}ê°œ")
    print(f"  - ë„¤ì´ë²„ ëª¨ë°”ì¼: {all_results['sources']['naver_mobile']['count']}ê°œ")
    print(f"  - ë‹¤ìŒ: {all_results['sources']['daum']['count']}ê°œ")
    print(f"  - êµ¬ê¸€ RSS: {all_results['sources']['google_rss']['count']}ê°œ")
    
    communities = all_results['sources']['communities']
    if communities:
        print(f"  - ì»¤ë®¤ë‹ˆí‹°: {len(communities)}ê°œ ì‚¬ì´íŠ¸")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = f"advanced_crawl_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    
    return all_results

if __name__ == "__main__":
    asyncio.run(test_advanced_crawler())
