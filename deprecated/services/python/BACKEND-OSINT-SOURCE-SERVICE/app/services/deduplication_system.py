"""
ë°ì´í„° ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ
PRD ê¸°ë°˜ êµ¬í˜„ - SimHashì™€ Bloom Filter í™œìš©
"""

import hashlib
import json
import re
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class URLNormalizer:
    """URL ì •ê·œí™”"""
    
    @staticmethod
    def normalize(url: str) -> str:
        """
        URL ì •ê·œí™”
        - ì†Œë¬¸ì ë³€í™˜
        - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬
        - Fragment ì œê±°
        - ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±°
        """
        if not url:
            return ""
        
        try:
            # URL íŒŒì‹±
            parsed = urlparse(url.lower())
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬
            query_params = parse_qs(parsed.query)
            sorted_query = urlencode(sorted(query_params.items()), doseq=True)
            
            # Path ì •ê·œí™” (ë ìŠ¬ë˜ì‹œ ì œê±°, ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±°)
            path = re.sub(r'/+', '/', parsed.path)
            path = path.rstrip('/')
            
            # ì¬ì¡°í•©
            normalized = urlunparse((
                parsed.scheme,
                parsed.netloc,
                path,
                parsed.params,
                sorted_query,
                ''  # Fragment ì œê±°
            ))
            
            return normalized
            
        except Exception as e:
            logger.error(f"URL ì •ê·œí™” ì‹¤íŒ¨: {url} - {e}")
            return url


class SimHash:
    """SimHash êµ¬í˜„"""
    
    def __init__(self, value: str, hashbits: int = 64):
        self.hashbits = hashbits
        self.value = value
        self.hash = self._calculate_hash(value)
    
    def _calculate_hash(self, content: str) -> int:
        """SimHash ê³„ì‚°"""
        # í† í°í™”
        tokens = self._tokenize(content)
        
        # ê° í† í°ì˜ í•´ì‹œ ê³„ì‚°
        v = [0] * self.hashbits
        
        for token in tokens:
            # í† í° í•´ì‹œ
            token_hash = int(hashlib.md5(token.encode()).hexdigest(), 16)
            
            # ë¹„íŠ¸ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            for i in range(self.hashbits):
                bitmask = 1 << i
                if token_hash & bitmask:
                    v[i] += 1
                else:
                    v[i] -= 1
        
        # ìµœì¢… í•´ì‹œ ìƒì„±
        fingerprint = 0
        for i in range(self.hashbits):
            if v[i] >= 0:
                fingerprint |= 1 << i
        
        return fingerprint
    
    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        # í•œêµ­ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í† í°í™”
        text = text.lower()
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()
        # 2-gram ìƒì„±
        ngrams = []
        for i in range(len(tokens) - 1):
            ngrams.append(tokens[i] + tokens[i + 1])
        return tokens + ngrams
    
    def hamming_distance(self, other: 'SimHash') -> int:
        """í•´ë° ê±°ë¦¬ ê³„ì‚°"""
        x = self.hash ^ other.hash
        distance = 0
        while x:
            distance += 1
            x &= x - 1
        return distance


class BloomFilter:
    """Bloom Filter êµ¬í˜„"""
    
    def __init__(self, capacity: int = 1000000, error_rate: float = 0.001):
        """
        Args:
            capacity: ì˜ˆìƒ í•­ëª© ìˆ˜
            error_rate: False positive í™•ë¥ 
        """
        # ìµœì  í¬ê¸° ê³„ì‚°
        self.size = self._optimal_size(capacity, error_rate)
        self.hash_count = self._optimal_hash_count(self.size, capacity)
        
        # ë¹„íŠ¸ ë°°ì—´
        self.bit_array = [False] * self.size
        self.count = 0
        
        logger.info(f"Bloom Filter ìƒì„±: size={self.size}, hash_count={self.hash_count}")
    
    def _optimal_size(self, n: int, p: float) -> int:
        """ìµœì  ë¹„íŠ¸ ë°°ì—´ í¬ê¸° ê³„ì‚°"""
        import math
        m = -(n * math.log(p)) / (math.log(2) ** 2)
        return int(m)
    
    def _optimal_hash_count(self, m: int, n: int) -> int:
        """ìµœì  í•´ì‹œ í•¨ìˆ˜ ê°œìˆ˜ ê³„ì‚°"""
        import math
        k = (m / n) * math.log(2)
        return int(k)
    
    def _hash(self, item: str, seed: int) -> int:
        """í•´ì‹œ ê³„ì‚°"""
        hash_value = hashlib.md5(f"{item}{seed}".encode()).hexdigest()
        return int(hash_value, 16) % self.size
    
    def add(self, item: str):
        """í•­ëª© ì¶”ê°€"""
        for i in range(self.hash_count):
            index = self._hash(item, i)
            self.bit_array[index] = True
        self.count += 1
    
    def contains(self, item: str) -> bool:
        """í•­ëª© ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        for i in range(self.hash_count):
            index = self._hash(item, i)
            if not self.bit_array[index]:
                return False
        return True
    
    def __contains__(self, item: str) -> bool:
        return self.contains(item)


class DeduplicationEngine:
    """
    ì¤‘ë³µ ì œê±° ì—”ì§„
    PRD ëª…ì„¸: < 2% ì¤‘ë³µë¥  ëª©í‘œ
    """
    
    def __init__(self, similarity_threshold: int = 3):
        """
        Args:
            similarity_threshold: SimHash í•´ë° ê±°ë¦¬ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ì—„ê²©)
        """
        # URL ì¤‘ë³µ í™•ì¸ìš© Bloom Filter
        self.url_bloom = BloomFilter(capacity=1000000, error_rate=0.001)
        self.url_cache = set()  # ì •í™•í•œ í™•ì¸ìš©
        
        # ì½˜í…ì¸  ì¤‘ë³µ í™•ì¸ìš© SimHash
        self.simhash_index = {}  # hash -> content_id
        self.similarity_threshold = similarity_threshold
        
        # URL ì •ê·œí™”
        self.url_normalizer = URLNormalizer()
        
        # í†µê³„
        self.stats = {
            'total_checked': 0,
            'duplicates_found': 0,
            'near_duplicates_found': 0,
            'unique_items': 0
        }
    
    def is_duplicate_url(self, url: str) -> bool:
        """URL ì¤‘ë³µ í™•ì¸"""
        # ì •ê·œí™”
        normalized_url = self.url_normalizer.normalize(url)
        
        # Bloom Filterë¡œ ë¹ ë¥¸ í™•ì¸
        if normalized_url in self.url_bloom:
            # ì •í™•í•œ í™•ì¸
            if normalized_url in self.url_cache:
                self.stats['duplicates_found'] += 1
                logger.debug(f"ì¤‘ë³µ URL ê°ì§€: {normalized_url}")
                return True
        
        # ìƒˆë¡œìš´ URL ë“±ë¡
        self.url_bloom.add(normalized_url)
        self.url_cache.add(normalized_url)
        
        return False
    
    def is_duplicate_content(self, content: str, content_id: str = None) -> Tuple[bool, Optional[str]]:
        """
        ì½˜í…ì¸  ì¤‘ë³µ í™•ì¸ (Near-duplicate í¬í•¨)
        
        Returns:
            (is_duplicate, similar_content_id)
        """
        if not content:
            return False, None
        
        # SimHash ìƒì„±
        simhash = SimHash(content)
        
        # Near-duplicate ê²€ì‚¬
        for existing_hash, existing_id in self.simhash_index.items():
            existing_simhash = SimHash(content)
            existing_simhash.hash = existing_hash
            
            distance = simhash.hamming_distance(existing_simhash)
            
            if distance <= self.similarity_threshold:
                self.stats['near_duplicates_found'] += 1
                logger.debug(f"Near-duplicate ê°ì§€: í•´ë° ê±°ë¦¬={distance}, ID={existing_id}")
                return True, existing_id
        
        # ìƒˆë¡œìš´ ì½˜í…ì¸  ë“±ë¡
        if content_id:
            self.simhash_index[simhash.hash] = content_id
        else:
            content_id = hashlib.md5(content.encode()).hexdigest()[:16]
            self.simhash_index[simhash.hash] = content_id
        
        self.stats['unique_items'] += 1
        return False, None
    
    def check_duplicate(self, item: Dict) -> Dict:
        """
        í†µí•© ì¤‘ë³µ í™•ì¸
        
        Args:
            item: {
                'url': str,
                'content': str,
                'title': str,
                'id': str (optional)
            }
        
        Returns:
            {
                'is_duplicate': bool,
                'duplicate_type': 'url' | 'content' | 'near_duplicate' | None,
                'similar_id': str (if near-duplicate),
                'confidence': float
            }
        """
        self.stats['total_checked'] += 1
        
        # URL ì¤‘ë³µ í™•ì¸
        if 'url' in item:
            if self.is_duplicate_url(item['url']):
                return {
                    'is_duplicate': True,
                    'duplicate_type': 'url',
                    'similar_id': None,
                    'confidence': 1.0
                }
        
        # ì½˜í…ì¸  ì¤‘ë³µ í™•ì¸
        if 'content' in item:
            is_dup, similar_id = self.is_duplicate_content(
                item['content'], 
                item.get('id')
            )
            
            if is_dup:
                return {
                    'is_duplicate': True,
                    'duplicate_type': 'near_duplicate' if similar_id else 'content',
                    'similar_id': similar_id,
                    'confidence': 0.95  # Near-duplicateëŠ” ì‹ ë¢°ë„ ì•½ê°„ ë‚®ì¶¤
                }
        
        # ì œëª© ê¸°ë°˜ ì¶”ê°€ í™•ì¸ (ì„ íƒì )
        if 'title' in item:
            title_hash = hashlib.md5(item['title'].encode()).hexdigest()
            # ì œëª©ì€ Bloom Filterë§Œ ì‚¬ìš© (ë¹ ë¥¸ í™•ì¸)
            if title_hash in self.url_bloom:  # URL bloomì„ ì¬ì‚¬ìš©
                logger.warning(f"ì œëª© ì¤‘ë³µ ê°€ëŠ¥ì„±: {item['title']}")
        
        return {
            'is_duplicate': False,
            'duplicate_type': None,
            'similar_id': None,
            'confidence': 0.0
        }
    
    def deduplicate_list(self, items: List[Dict]) -> List[Dict]:
        """
        ë¦¬ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±°
        
        Args:
            items: ì¤‘ë³µ ì œê±°í•  í•­ëª© ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ ë¦¬ìŠ¤íŠ¸
        """
        unique_items = []
        
        for item in items:
            result = self.check_duplicate(item)
            
            if not result['is_duplicate']:
                unique_items.append(item)
            else:
                logger.debug(
                    f"ì¤‘ë³µ ì œê±°: {item.get('title', 'Unknown')} "
                    f"(type={result['duplicate_type']})"
                )
        
        logger.info(
            f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(items)} â†’ {len(unique_items)} "
            f"({len(items) - len(unique_items)} ì œê±°)"
        )
        
        return unique_items
    
    def get_statistics(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'duplicate_rate': (
                self.stats['duplicates_found'] / 
                max(self.stats['total_checked'], 1) * 100
            ),
            'near_duplicate_rate': (
                self.stats['near_duplicates_found'] / 
                max(self.stats['total_checked'], 1) * 100
            ),
            'unique_rate': (
                self.stats['unique_items'] / 
                max(self.stats['total_checked'], 1) * 100
            ),
            'bloom_filter_size': self.url_bloom.size,
            'simhash_index_size': len(self.simhash_index)
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
        # URL ìºì‹œëŠ” ì¼ì • í¬ê¸° ì´ìƒì¼ ë•Œë§Œ ì •ë¦¬
        if len(self.url_cache) > 100000:
            # ì˜¤ë˜ëœ í•­ëª© ì œê±° (ê°„ë‹¨íˆ ì ˆë°˜ë§Œ ìœ ì§€)
            keep_size = 50000
            self.url_cache = set(list(self.url_cache)[-keep_size:])
            logger.info(f"URL ìºì‹œ ì •ë¦¬: {keep_size}ê°œ ìœ ì§€")
        
        # SimHash ì¸ë±ìŠ¤ë„ í¬ê¸° ì œí•œ
        if len(self.simhash_index) > 50000:
            # ì˜¤ë˜ëœ í•­ëª© ì œê±°
            keep_size = 25000
            items = list(self.simhash_index.items())
            self.simhash_index = dict(items[-keep_size:])
            logger.info(f"SimHash ì¸ë±ìŠ¤ ì •ë¦¬: {keep_size}ê°œ ìœ ì§€")


# í…ŒìŠ¤íŠ¸
def test_deduplication_engine():
    """ì¤‘ë³µ ì œê±° ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    
    engine = DeduplicationEngine(similarity_threshold=3)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_items = [
        {
            'url': 'https://www.naver.com/news/article1',
            'title': 'êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ë°©ì•ˆ ë°œí‘œ',
            'content': 'ì •ë¶€ëŠ” ì˜¤ëŠ˜ êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ë°©ì•ˆì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ë³´í—˜ë£Œìœ¨ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì¸ìƒí•˜ê³ ...'
        },
        {
            'url': 'https://www.naver.com/news/article1',  # ë™ì¼ URL
            'title': 'êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ë°©ì•ˆ ë°œí‘œ',
            'content': 'ì •ë¶€ëŠ” ì˜¤ëŠ˜ êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ë°©ì•ˆì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ë³´í—˜ë£Œìœ¨ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì¸ìƒí•˜ê³ ...'
        },
        {
            'url': 'https://www.daum.net/news/article2',
            'title': 'êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ ë°©ì•ˆ ë°œí‘œ',  # ë™ì¼ ì œëª©, ë‹¤ë¥¸ URL
            'content': 'ì •ë¶€ëŠ” ì˜¤ëŠ˜ êµ­ë¯¼ì—°ê¸ˆ ê°œí˜ì•ˆì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ë³´í—˜ë£Œìœ¨ì„ ì ì§„ì ìœ¼ë¡œ ì˜¬ë¦¬ê³ ...'  # ìœ ì‚¬ ë‚´ìš©
        },
        {
            'url': 'https://www.joins.com/news/article3',
            'title': 'ë‚ ì”¨ ì •ë³´',
            'content': 'ì˜¤ëŠ˜ ì „êµ­ì´ ë§‘ì€ ë‚ ì”¨ë¥¼ ë³´ì´ê² ìŠµë‹ˆë‹¤.'  # ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©
        },
        {
            'url': 'https://www.NAVER.com/news/article1?param=1&param=2',  # ì •ê·œí™” í•„ìš”
            'title': 'êµ­ë¯¼ì—°ê¸ˆ ê°œí˜',
            'content': 'ê°œí˜ ë‚´ìš©...'
        }
    ]
    
    print("=" * 60)
    print("ì¤‘ë³µ ì œê±° ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    unique_items = []
    
    for i, item in enumerate(test_items, 1):
        print(f"\ní•­ëª© {i}: {item['title']}")
        print(f"  URL: {item['url']}")
        
        result = engine.check_duplicate(item)
        
        if result['is_duplicate']:
            print(f"  âŒ ì¤‘ë³µ: {result['duplicate_type']}")
            if result['similar_id']:
                print(f"  ìœ ì‚¬ ID: {result['similar_id']}")
        else:
            print(f"  âœ… ê³ ìœ  í•­ëª©")
            unique_items.append(item)
    
    # í†µê³„ ì¶œë ¥
    stats = engine.get_statistics()
    print(f"\nğŸ“Š í†µê³„:")
    print(f"  ê²€ì‚¬: {stats['total_checked']}ê±´")
    print(f"  ì¤‘ë³µ: {stats['duplicates_found']}ê±´ ({stats['duplicate_rate']:.1f}%)")
    print(f"  ìœ ì‚¬: {stats['near_duplicates_found']}ê±´ ({stats['near_duplicate_rate']:.1f}%)")
    print(f"  ê³ ìœ : {stats['unique_items']}ê±´ ({stats['unique_rate']:.1f}%)")
    
    print(f"\nâœ… ìµœì¢… ê³ ìœ  í•­ëª©: {len(unique_items)}ê°œ")


if __name__ == "__main__":
    test_deduplication_engine()
