"""
í•œêµ­ì–´ NLP ê¸°ë°˜ ê´€ë ¨ì„± í•„í„°ë§ ì‹œìŠ¤í…œ
PRD ê¸°ë°˜ êµ¬í˜„ - 85% ì´ìƒ ì •í™•ë„ ëª©í‘œ
"""

import re
import hashlib
import json
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

# NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from konlpy.tag import Okt, Kkma, Komoran
except ImportError:
    print("KoNLPy ì„¤ì¹˜ í•„ìš”: pip install konlpy")
    Okt = Kkma = Komoran = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RelevanceScore:
    """ê´€ë ¨ì„± ì ìˆ˜"""
    overall_score: float
    keyword_score: float
    entity_score: float
    semantic_score: float
    is_relevant: bool
    confidence: float
    reason: str


class KoreanTextNormalizer:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    
    def __init__(self):
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´
        self.noise_patterns = [
            r'\[.*?\]',  # ëŒ€ê´„í˜¸ ë‚´ìš©
            r'\(.*?\)',  # ì†Œê´„í˜¸ ë‚´ìš©
            r'<.*?>',    # HTML íƒœê·¸
            r'http[s]?://[^\s]+',  # URL
            r'[^\w\sê°€-í£]',  # íŠ¹ìˆ˜ë¬¸ì (í•œê¸€, ì˜ë¬¸, ìˆ«ì ì œì™¸)
        ]
        
        # í•œêµ­ì–´ ì¡°ì‚¬
        self.particles = [
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 
            'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 
            'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜'
        ]
    
    def normalize(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ""
        
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # ë…¸ì´ì¦ˆ ì œê±°
        for pattern in self.noise_patterns:
            text = re.sub(pattern, ' ', text)
        
        # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def remove_particles(self, text: str) -> str:
        """ì¡°ì‚¬ ì œê±°"""
        words = text.split()
        processed_words = []
        
        for word in words:
            # ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° ì œê±°
            for particle in self.particles:
                if word.endswith(particle):
                    word = word[:-len(particle)]
                    break
            processed_words.append(word)
        
        return ' '.join(processed_words)


class KoreanEntityExtractor:
    """í•œêµ­ì–´ ê°œì²´ëª… ì¶”ì¶œ"""
    
    def __init__(self):
        self.morpheme_analyzer = Okt() if Okt else None
        
        # êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ í•µì‹¬ ê°œì²´
        self.pension_entities = [
            'êµ­ë¯¼ì—°ê¸ˆ', 'êµ­ì—°', 'ì—°ê¸ˆ', 'ë…¸í›„', 'ì€í‡´', 'í‡´ì§',
            'ë³´í—˜ë£Œ', 'ë‚©ë¶€', 'ìˆ˜ê¸‰', 'ê¸‰ì—¬', 'ì—°ê¸ˆì•¡', 'ì†Œë“ëŒ€ì²´ìœ¨',
            'êµ­ë¯¼ì—°ê¸ˆê³µë‹¨', 'ë³´ê±´ë³µì§€ë¶€', 'ë³µì§€ë¶€', 'NPS'
        ]
        
        # ê¸ˆìœµ ê´€ë ¨ ê°œì²´
        self.finance_entities = [
            'íˆ¬ì', 'ìˆ˜ìµë¥ ', 'ìš´ìš©', 'ìì‚°', 'ê¸°ê¸ˆ', 'ì£¼ì‹',
            'ì±„ê¶Œ', 'ë¶€ë™ì‚°', 'í¬íŠ¸í´ë¦¬ì˜¤', 'ë¦¬ìŠ¤í¬'
        ]
        
        # ì •ì±… ê´€ë ¨ ê°œì²´
        self.policy_entities = [
            'ê°œí˜', 'ì •ì±…', 'ë²•ì•ˆ', 'ì œë„', 'ê°œì„ ', 'ì¸ìƒ',
            'ê°ì†Œ', 'í™•ëŒ€', 'ì¶•ì†Œ', 'ë³€ê²½'
        ]
    
    def extract_entities(self, text: str) -> List[Dict]:
        """ê°œì²´ëª… ì¶”ì¶œ"""
        entities = []
        
        # ì •ê·œí™”
        normalizer = KoreanTextNormalizer()
        normalized_text = normalizer.normalize(text)
        
        # êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ê°œì²´ ê²€ìƒ‰
        for entity in self.pension_entities:
            if entity in normalized_text:
                entities.append({
                    'text': entity,
                    'type': 'PENSION',
                    'category': 'ì—°ê¸ˆ'
                })
        
        # ê¸ˆìœµ ê´€ë ¨ ê°œì²´ ê²€ìƒ‰
        for entity in self.finance_entities:
            if entity in normalized_text:
                entities.append({
                    'text': entity,
                    'type': 'FINANCE',
                    'category': 'ê¸ˆìœµ'
                })
        
        # ì •ì±… ê´€ë ¨ ê°œì²´ ê²€ìƒ‰
        for entity in self.policy_entities:
            if entity in normalized_text:
                entities.append({
                    'text': entity,
                    'type': 'POLICY',
                    'category': 'ì •ì±…'
                })
        
        # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì¶”ê°€ ê°œì²´ ì¶”ì¶œ
        if self.morpheme_analyzer:
            try:
                nouns = self.morpheme_analyzer.nouns(normalized_text)
                for noun in nouns:
                    if len(noun) >= 2:  # 2ê¸€ì ì´ìƒ ëª…ì‚¬
                        # ì¤‘ë³µ í™•ì¸
                        if not any(e['text'] == noun for e in entities):
                            entities.append({
                                'text': noun,
                                'type': 'NOUN',
                                'category': 'ì¼ë°˜ëª…ì‚¬'
                            })
            except:
                pass
        
        return entities


class SemanticSimilarityCalculator:
    """ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    
    def __init__(self):
        self.normalizer = KoreanTextNormalizer()
        
        # êµ­ë¯¼ì—°ê¸ˆ ê´€ë ¨ ì‹œë§¨í‹± ê·¸ë£¹
        self.semantic_groups = {
            'ì—°ê¸ˆìˆ˜ê¸‰': ['ìˆ˜ê¸‰', 'ê¸‰ì—¬', 'ì§€ê¸‰', 'ìˆ˜ë ¹', 'ì—°ê¸ˆì•¡'],
            'ë³´í—˜ë£Œ': ['ë‚©ë¶€', 'ë³´í—˜ë£Œ', 'ê¸°ì—¬ê¸ˆ', 'ë¶€ë‹´ê¸ˆ'],
            'íˆ¬ììš´ìš©': ['íˆ¬ì', 'ìš´ìš©', 'ìˆ˜ìµë¥ ', 'í¬íŠ¸í´ë¦¬ì˜¤', 'ìì‚°ë°°ë¶„'],
            'ì •ì±…ê°œí˜': ['ê°œí˜', 'ê°œì„ ', 'ì •ì±…', 'ì œë„', 'ë³€ê²½'],
            'ë…¸í›„ì¤€ë¹„': ['ë…¸í›„', 'ì€í‡´', 'í‡´ì§', 'ì¤€ë¹„', 'ëŒ€ë¹„']
        }
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        # ì •ê·œí™”
        norm_text1 = self.normalizer.normalize(text1)
        norm_text2 = self.normalizer.normalize(text2)
        
        # ë‹¨ì–´ ì§‘í•© ìƒì„±
        words1 = set(norm_text1.split())
        words2 = set(norm_text2.split())
        
        # Jaccard ìœ ì‚¬ë„
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        jaccard_score = len(intersection) / len(union) if union else 0
        
        # ì‹œë§¨í‹± ê·¸ë£¹ ê¸°ë°˜ ìœ ì‚¬ë„
        semantic_score = self._calculate_semantic_group_similarity(words1, words2)
        
        # ê°€ì¤‘ í‰ê· 
        final_score = (jaccard_score * 0.6) + (semantic_score * 0.4)
        
        return min(1.0, final_score)
    
    def _calculate_semantic_group_similarity(self, words1: set, words2: set) -> float:
        """ì‹œë§¨í‹± ê·¸ë£¹ ê¸°ë°˜ ìœ ì‚¬ë„"""
        score = 0.0
        count = 0
        
        for group_name, group_words in self.semantic_groups.items():
            group_set = set(group_words)
            
            # ë‘ í…ìŠ¤íŠ¸ê°€ ê°™ì€ ì‹œë§¨í‹± ê·¸ë£¹ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            matches1 = words1 & group_set
            matches2 = words2 & group_set
            
            if matches1 and matches2:
                score += 1.0
                count += 1
            elif matches1 or matches2:
                score += 0.5
                count += 1
        
        return score / count if count > 0 else 0.0


class KoreanRelevanceFilter:
    """
    í•œêµ­ì–´ ê´€ë ¨ì„± í•„í„°ë§ ì—”ì§„
    PRD ëª…ì„¸: 85% ì´ìƒ ì •í™•ë„ ëª©í‘œ
    """
    
    def __init__(self):
        self.normalizer = KoreanTextNormalizer()
        self.entity_extractor = KoreanEntityExtractor()
        self.similarity_calculator = SemanticSimilarityCalculator()
        
        # ê´€ë ¨ì„± ì„ê³„ê°’
        self.relevance_threshold = 0.75  # PRD: 75% ì´ìƒ
        
        # í•„í„°ë§ í†µê³„
        self.stats = {
            'total_processed': 0,
            'relevant_count': 0,
            'filtered_count': 0,
            'avg_relevance_score': 0.0
        }
    
    def calculate_relevance(
        self, 
        content: str, 
        search_term: str,
        reference_content: Optional[str] = None
    ) -> RelevanceScore:
        """
        ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            content: í‰ê°€í•  ì½˜í…ì¸ 
            search_term: ê²€ìƒ‰ì–´
            reference_content: ì°¸ì¡° ì½˜í…ì¸  (ë‰´ìŠ¤ ë“±)
        
        Returns:
            RelevanceScore ê°ì²´
        """
        
        if not content or not search_term:
            return RelevanceScore(
                overall_score=0.0,
                keyword_score=0.0,
                entity_score=0.0,
                semantic_score=0.0,
                is_relevant=False,
                confidence=0.0,
                reason="ì½˜í…ì¸  ë˜ëŠ” ê²€ìƒ‰ì–´ ì—†ìŒ"
            )
        
        # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_score = self._calculate_keyword_score(content, search_term)
        
        # 2. ê°œì²´ëª… ë§¤ì¹­ ì ìˆ˜
        entity_score = self._calculate_entity_score(content, search_term)
        
        # 3. ì˜ë¯¸ ìœ ì‚¬ë„ ì ìˆ˜
        semantic_score = self.similarity_calculator.calculate_similarity(content, search_term)
        
        # 4. ì°¸ì¡° ì½˜í…ì¸ ì™€ì˜ ìœ ì‚¬ë„ (ìˆëŠ” ê²½ìš°)
        reference_score = 0.0
        if reference_content:
            reference_score = self.similarity_calculator.calculate_similarity(content, reference_content)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        if reference_content:
            overall_score = (
                keyword_score * 0.25 +
                entity_score * 0.25 +
                semantic_score * 0.25 +
                reference_score * 0.25
            )
        else:
            overall_score = (
                keyword_score * 0.35 +
                entity_score * 0.35 +
                semantic_score * 0.30
            )
        
        # ê´€ë ¨ì„± íŒë‹¨
        is_relevant = overall_score >= self.relevance_threshold
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(1.0, overall_score * 1.2) if is_relevant else overall_score
        
        # íŒë‹¨ ì´ìœ 
        if is_relevant:
            reason = f"ê´€ë ¨ì„± ë†’ìŒ (ì ìˆ˜: {overall_score:.2f})"
        else:
            reason = f"ê´€ë ¨ì„± ë‚®ìŒ (ì ìˆ˜: {overall_score:.2f})"
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_statistics(overall_score, is_relevant)
        
        return RelevanceScore(
            overall_score=overall_score,
            keyword_score=keyword_score,
            entity_score=entity_score,
            semantic_score=semantic_score,
            is_relevant=is_relevant,
            confidence=confidence,
            reason=reason
        )
    
    def _calculate_keyword_score(self, content: str, search_term: str) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜"""
        # ì •ê·œí™”
        norm_content = self.normalizer.normalize(content)
        norm_search = self.normalizer.normalize(search_term)
        
        # ê²€ìƒ‰ì–´ ë¶„í• 
        search_words = norm_search.split()
        
        # ê° ê²€ìƒ‰ì–´ì˜ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
        total_score = 0.0
        for word in search_words:
            count = norm_content.count(word)
            # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš© (ë„ˆë¬´ ë§ì´ ë‚˜ì™€ë„ 1.0 ë„˜ì§€ ì•Šë„ë¡)
            word_score = min(1.0, count / 10.0)
            total_score += word_score
        
        # í‰ê·  ì ìˆ˜
        avg_score = total_score / len(search_words) if search_words else 0.0
        
        return min(1.0, avg_score)
    
    def _calculate_entity_score(self, content: str, search_term: str) -> float:
        """ê°œì²´ëª… ë§¤ì¹­ ì ìˆ˜"""
        # ì½˜í…ì¸ ì—ì„œ ê°œì²´ ì¶”ì¶œ
        content_entities = self.entity_extractor.extract_entities(content)
        search_entities = self.entity_extractor.extract_entities(search_term)
        
        if not search_entities:
            return 0.5  # ê²€ìƒ‰ì–´ì— íŠ¹ë³„í•œ ê°œì²´ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ ì ìˆ˜
        
        # ë§¤ì¹­ë˜ëŠ” ê°œì²´ ìˆ˜ ê³„ì‚°
        content_entity_texts = {e['text'] for e in content_entities}
        search_entity_texts = {e['text'] for e in search_entities}
        
        matches = content_entity_texts & search_entity_texts
        
        if not search_entity_texts:
            return 0.0
        
        # ë§¤ì¹­ ë¹„ìœ¨
        match_ratio = len(matches) / len(search_entity_texts)
        
        # ì¤‘ìš” ê°œì²´(PENSION) ê°€ì¤‘ì¹˜ ë¶€ì—¬
        important_matches = sum(
            1 for e in content_entities 
            if e['text'] in matches and e['type'] == 'PENSION'
        )
        
        if important_matches > 0:
            match_ratio = min(1.0, match_ratio * 1.5)
        
        return match_ratio
    
    def _update_statistics(self, score: float, is_relevant: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['total_processed'] += 1
        
        if is_relevant:
            self.stats['relevant_count'] += 1
        else:
            self.stats['filtered_count'] += 1
        
        # í‰ê·  ì ìˆ˜ ì—…ë°ì´íŠ¸
        n = self.stats['total_processed']
        prev_avg = self.stats['avg_relevance_score']
        self.stats['avg_relevance_score'] = (prev_avg * (n - 1) + score) / n
    
    def filter_articles(self, articles: List[Dict], search_term: str) -> List[Dict]:
        """
        ê¸°ì‚¬ ëª©ë¡ í•„í„°ë§
        
        Args:
            articles: ê¸°ì‚¬ ëª©ë¡
            search_term: ê²€ìƒ‰ì–´
        
        Returns:
            í•„í„°ë§ëœ ê¸°ì‚¬ ëª©ë¡
        """
        filtered_articles = []
        
        for article in articles:
            # URL ìœ íš¨ì„± í•„í„°ë§ (ì¡´ì¬í•˜ëŠ” ê²½ìš° http/httpsë§Œ í—ˆìš©)
            url = article.get('url', '')
            if url and not url.startswith(('http://', 'https://')):
                logger.debug(f"ë¬´íš¨ URLë¡œ í•„í„°ë§: {url}")
                continue
            # ì œëª©ê³¼ ë‚´ìš© ê²°í•©
            content = f"{article.get('title', '')} {article.get('content', '')}"
            
            # ê´€ë ¨ì„± ê³„ì‚°
            relevance = self.calculate_relevance(content, search_term)
            
            if relevance.is_relevant:
                # ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€
                article['relevance_score'] = relevance.overall_score
                article['relevance_confidence'] = relevance.confidence
                filtered_articles.append(article)
            else:
                logger.debug(f"í•„í„°ë§ë¨: {article.get('title', 'Unknown')} - {relevance.reason}")
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        filtered_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return filtered_articles
    
    def get_statistics(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'relevance_rate': (
                self.stats['relevant_count'] / max(self.stats['total_processed'], 1) * 100
            ),
            'filter_rate': (
                self.stats['filtered_count'] / max(self.stats['total_processed'], 1) * 100
            )
        }


# í…ŒìŠ¤íŠ¸
def test_korean_relevance_filter():
    """í•œêµ­ì–´ ê´€ë ¨ì„± í•„í„° í…ŒìŠ¤íŠ¸"""
    filter = KoreanRelevanceFilter()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            'content': 'êµ­ë¯¼ì—°ê¸ˆ ë³´í—˜ë£Œê°€ ì¸ìƒë  ì˜ˆì •ì…ë‹ˆë‹¤. ë…¸í›„ ì¤€ë¹„ë¥¼ ìœ„í•œ ì—°ê¸ˆ ê°œí˜ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'search_term': 'êµ­ë¯¼ì—°ê¸ˆ',
            'expected': True
        },
        {
            'content': 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤. ì˜í™”ë¥¼ ë³´ëŸ¬ ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤.',
            'search_term': 'êµ­ë¯¼ì—°ê¸ˆ',
            'expected': False
        },
        {
            'content': 'ì—°ê¸ˆ ìˆ˜ê¸‰ ì—°ë ¹ì´ 65ì„¸ë¡œ ìƒí–¥ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì€í‡´ ì¤€ë¹„ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.',
            'search_term': 'êµ­ë¯¼ì—°ê¸ˆ ìˆ˜ê¸‰',
            'expected': True
        },
        {
            'content': 'NATO ì •ìƒíšŒì˜ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. êµ­ì œ ì•ˆë³´ ë¬¸ì œë¥¼ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤.',
            'search_term': 'êµ­ë¯¼ì—°ê¸ˆ',
            'expected': False
        }
    ]
    
    print("=" * 60)
    print("í•œêµ­ì–´ ê´€ë ¨ì„± í•„í„° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for i, test in enumerate(test_cases, 1):
        result = filter.calculate_relevance(test['content'], test['search_term'])
        
        # ê²°ê³¼ ê²€ì¦
        success = result.is_relevant == test['expected']
        status = "âœ…" if success else "âŒ"
        
        print(f"\ní…ŒìŠ¤íŠ¸ {i}: {status}")
        print(f"  ì½˜í…ì¸ : {test['content'][:50]}...")
        print(f"  ê²€ìƒ‰ì–´: {test['search_term']}")
        print(f"  ì˜ˆìƒ: {test['expected']}, ì‹¤ì œ: {result.is_relevant}")
        print(f"  ì ìˆ˜: {result.overall_score:.2f}")
        print(f"    - í‚¤ì›Œë“œ: {result.keyword_score:.2f}")
        print(f"    - ê°œì²´ëª…: {result.entity_score:.2f}")
        print(f"    - ì˜ë¯¸: {result.semantic_score:.2f}")
        print(f"  ì´ìœ : {result.reason}")
    
    # í†µê³„ ì¶œë ¥
    stats = filter.get_statistics()
    print(f"\nğŸ“Š í†µê³„:")
    print(f"  ì²˜ë¦¬: {stats['total_processed']}ê±´")
    print(f"  ê´€ë ¨: {stats['relevant_count']}ê±´ ({stats['relevance_rate']:.1f}%)")
    print(f"  í•„í„°: {stats['filtered_count']}ê±´ ({stats['filter_rate']:.1f}%)")
    print(f"  í‰ê·  ì ìˆ˜: {stats['avg_relevance_score']:.2f}")


if __name__ == "__main__":
    test_korean_relevance_filter()
